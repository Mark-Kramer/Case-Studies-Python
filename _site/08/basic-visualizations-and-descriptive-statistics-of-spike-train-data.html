<!DOCTYPE html>
<html lang="en">
  

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Basic Visualizations and Descriptive Statistics of Spike Train Data</title>
  <meta name="description" content="">

  <link rel="canonical" href="https://eschlaf2.github.io/Case-Studies-Python/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data.html">
  <link rel="alternate" type="application/rss+xml" title="Case Studies in Neural Data Analysis" href="https://eschlaf2.github.io/Case-Studies-Python/feed.xml">

  <meta property="og:url"         content="https://eschlaf2.github.io/Case-Studies-Python/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Basic Visualizations and Descriptive Statistics of Spike Train Data" />
<meta property="og:description" content="" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "https://eschlaf2.github.io/Case-Studies-Python/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data.html",
  "headline":
    "Basic Visualizations and Descriptive Statistics of Spike Train Data",
  "datePublished":
    "2019-04-24T23:26:10-04:00",
  "dateModified":
    "2019-04-24T23:26:10-04:00",
  "description":
    "",
  "author": {
    "@type": "Person",
    "name": "Mark Kramer and Uri Eden"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://eschlaf2.github.io/Case-Studies-Python",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://eschlaf2.github.io/Case-Studies-Python",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/Case-Studies-Python/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/Case-Studies-Python/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/Case-Studies-Python';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/Case-Studies-Python/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/Case-Studies-Python/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  <script src="https://unpkg.com/nbinteract-core" async></script>

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->

<script type="text/x-thebe-config">
    {
      requestKernel: true,
      binderOptions: {
        repo: 'eschlaf2/Case-Studies-Python',
        ref: 'master',
      },
      kernelOptions: {
        name: 'python3',
      }
    }
</script>
<script src="https://unpkg.com/thebelab@0.3.3/lib/index.js"></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)
                codeCell.setAttribute('data-executable', 'true')

                // Figure out the language it uses and add this too
                var parentDiv = codeCell.parentElement.parentElement;
                var arrayLength = parentDiv.classList.length;
                for (var ii = 0; ii < arrayLength; ii++) {
                    var parts = parentDiv.classList[ii].split('language-');
                    if (parts.length === 2) {
                        // If found, assign dataLanguage and break the loop
                        var dataLanguage = parts[1];
                        break;
                    }
                }
                codeCell.setAttribute('data-language', dataLanguage)

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyButtons = document.querySelectorAll('.copybtn')
            copyButtons.forEach((copyButton, index) => {
                copyButton.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButton = document.getElementById('interact-button-thebelab');
        if (thebelabButton === null) {
            setTimeout(initThebelab, 250)
        return
        };
        thebelabButton.addEventListener('click', addThebelabToCodeCells);
    }

    // Initialize Thebelab
    initFunction(initThebelab);
</script>


  <!-- Google analytics -->
  <script src="/Case-Studies-Python/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/Case-Studies-Python/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.highlighter-rouge:not(.output) pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.querySelector(`pre#${id} + a`) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>

  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id}`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area div.highlight').forEach(function (item, index) {
    if (!item.parentElement.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    item.insertAdjacentHTML('afterend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/Case-Studies-Python/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/Case-Studies-Python/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/Case-Studies-Python/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("https://eschlaf2.github.io") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/Case-Studies-Python/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://jupyter.org/jupyter-book/intro.html"><img src="/Case-Studies-Python/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">Case Studies in Neural Data Analysis</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/Case-Studies-Python/intro.html"
        >
          
          Home
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__chapter"><a class="c-sidebar__entry" href="/Case-Studies-Python/search.html">Search</a></li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://github.com/Mark-Kramer/Case-Studies-Python.git"
        >
          
          GitHub repository
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__divider"></li>
        
      
      
        <li><h2 class="c-sidebar__title">Contents</li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/Case-Studies-Python/01/intro.html"
        >
          
            1.
          
          Introduction to Python
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/Case-Studies-Python/02/the-event-related-potential.html"
        >
          
            2.
          
          The Event-Related Potential
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/Case-Studies-Python/03/the-power-spectrum-part-1.html"
        >
          
            3.
          
          The Power Spectrum (Part 1)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/Case-Studies-Python/03/supplement-1.html"
                >
                  
                    3.1
                  
                  Biased versus unbiased autocovariance
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/Case-Studies-Python/03/supplement-2.html"
                >
                  
                    3.2
                  
                  Intuition behind the power spectral density
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/Case-Studies-Python/04/the-cross-covariance-and-coherence.html"
        >
          
            4.
          
          The Cross Covariance and Coherence
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/Case-Studies-Python/05/cross-frequency-coupling.html"
        >
          
            5.
          
          Cross Frequency Coupling
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/Case-Studies-Python/06/the-power-spectrum-part-2.html"
        >
          
            6.
          
          The Power Spectrum (Part 2)
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry c-sidebar__entry--active"
          href="/Case-Studies-Python/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data.html"
        >
          
            7.
          
          Basic Visualizations and Descriptive Statistics of Spike Train Data
        </a>

        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <!-- Shamelessly copied from minimal mistakes -->


<!-- TOC will only show up if it has at least one item -->


  <aside class="sidebar__right">
    <nav class="onthispage">
      <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
      <ul class="toc__menu">
  <li><a href="#introduction">Introduction</a>
    <ul>
      <li><a href="#case-study-data">Case Study Data</a></li>
      <li><a href="#goal">Goal</a></li>
      <li><a href="#tools">Tools</a></li>
    </ul>
  </li>
  <li><a href="#data-analysis">Data Analysis</a>
    <ul>
      <li><a href="#examining-the-interspike-intervals">Examining the Interspike Intervals</a></li>
      <li><a href="#examining-binned-spike-increments">Examining Binned Spike Increments</a></li>
      <li><a href="#computing-autocorrelations-for-the-increments">Computing Autocorrelations for the Increments</a></li>
      <li><a href="#computing-autocorrelations-of-the-isis">Computing Autocorrelations of the ISIs</a></li>
      <li><a href="#building-statistical-models-of-the-isis">Building Statistical Models of the ISIs</a></li>
    </ul>
  </li>
  <li><a href="#summary">Summary</a></li>
  <li><a href="#appendix-spike-count-mean-and-variance-for-a-poisson-process">Appendix: Spike Count Mean and Variance for a Poisson Process</a></li>
</ul>
    </nav>
  </aside>


      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            
<div class="buttons">
<a href="/Case-Studies-Python/content/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data.ipynb" download>
<button id="interact-button-download" class="interact-button">Download</button>
</a>

<button id="interact-button-thebelab" class="interact-button">Thebelab</button>








<a href="https://mybinder.org/v2/gh/eschlaf2/Case-Studies-Python/master?filepath=content%2F08%2Fbasic-visualizations-and-descriptive-statistics-of-spike-train-data.ipynb"><button class="interact-button" id="interact-button-binder"><img class="interact-button-logo" src="/Case-Studies-Python/assets/images/logo_binder.svg" alt="Interact" />Interact</button></a>


</div>


            <div class="c-textbook__content">
              <p><a id="top"></a></p>

<h1 id="basic-visualizations-and-descriptive-statistics-of-spike-train-data">Basic Visualizations and Descriptive Statistics of Spike Train Data</h1>

<div class="question">

  <p><em><strong>Synopsis</strong></em></p>

  <p><strong>Data:</strong> Spontaneous spiking activity from a retinal neuron in culture, exposed to low-light and high-light environments.</p>

  <p><strong>Goal:</strong> Visualize spike trains, compute and interpret descriptive statistics, and build simple models of interspike interval distributions as a function of the ambient light level.</p>

  <p><strong>Tools:</strong> Raster plots, interspike interval histograms, firing rate, autocorrelograms, maximum likelihood estimation, Kolmogorov-Smirnov plots.</p>
</div>

<ul>
  <li><a href="#.">Introduction</a></li>
  <li><a href="#data-analysis">Data analysis</a>
    <ol>
      <li><a href="#visual-inspection">Visual inspection</a></li>
      <li><a href="#isi">Examining the Interspike Intervals</a></li>
      <li><a href="#bsi">Examining Binned Spike Increments</a></li>
      <li><a href="#autocorrelations">Computing autocorrelations for the Increments</a></li>
      <li><a href="#acISI">Computing Autocorrelations of the ISIs</a></li>
      <li><a href="#models">Building Statistical Models of the ISIs</a></li>
    </ol>
  </li>
  <li><a href="#summary">Summary</a></li>
  <li><a href="#appendix">Appendix: Spike Count Mean and Variance for a Poisson Process</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Neurons in the retina typically respond to patterns of light displayed over small sections of the visual field. However, when retinal neurons are grown in culture and held under constant light and environmental conditions, they will still spontaneously fire action potentials. In a fully functioning retina, this spontaneous activity is sometimes described as background firing activity, which is modulated as a function of visual stimuli. It is useful to understand the properties of this background activity in order to determine in future experiments how these firing properties are affected by specific stimuli.</p>

<h3 id="case-study-data">Case Study Data</h3>

<p>A researcher examining the background firing properties of one of these neurons contacts you to discuss his data. He records the spiking activity in one of two states, with the room lights off (low ambient light levels) or with the room lights on (high ambient light levels). He would like to collaborate with you to determine whether there is a difference in background firing between these two conditions, and whether one environment is more conducive to future experimental analyses. He records the spiking activity for 30 seconds in each condition.</p>

<h3 id="goal">Goal</h3>

<p>Typically the first step in any data analysis involves visualizing and using simple descriptive statistics to characterize pertinent features of the data. For time series data that take on a continuous value at each time point, like the field potentials analyzed in earlier chapters, we typically start by simply plotting each data value as a function of time. For spike train data, things can become a bit more complicated. One reason for this is that there are multiple equivalent ways to describe the same spike train data. The data could be stored as a sequence of spike times; as a sequence of waiting times between spikes, or interspike intervals; or as a discrete time series indicating the number of spikes in discrete time bins. Knowing how to manipulate and visualize spike train data using all these different representations is the first step to understanding the structure present in the data and is the primary goal of this chapter.</p>

<h3 id="tools">Tools</h3>

<p>We develop tools in this chapter to visualize spike train data and to provide basic statistical methods appropriate for analyzing spike trains.</p>

<p><a id="data-analysis"></a></p>

<h2 id="data-analysis">Data Analysis</h2>

<p><a id="visual-inspection"></a></p>

<p>Our data analysis begins with visual inspection. We load the ECoG data into Python and plot them by issuing the following commands:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Prepare the modules and plot settings</span>
<span class="kn">import</span> <span class="nn">scipy.io</span> <span class="k">as</span> <span class="n">sio</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">import</span> <span class="n">xlabel</span><span class="p">,</span> <span class="n">ylabel</span><span class="p">,</span> <span class="n">plot</span><span class="p">,</span> <span class="n">show</span><span class="p">,</span> <span class="n">title</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">rcParams</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="nb">FutureWarning</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">sio</span><span class="o">.</span><span class="n">loadmat</span><span class="p">(</span><span class="s">'Ch8-spikes-1.mat'</span><span class="p">)</span>  <span class="c"># Load the ECoG data</span>
</code></pre></div></div>

<div class="question">

  <p><strong>Q.</strong> How can we extract the variables of interest from <code class="highlighter-rouge">data</code>? Hint: Consider the <code class="highlighter-rouge">keys()</code> method.</p>

</div>

<p>You should find two non-private variables in the <code class="highlighter-rouge">data</code> dictionary:</p>

<p><code class="highlighter-rouge">SpikesLow</code>: spike times over 30 s in the low ambient light condition
<code class="highlighter-rouge">SpikesHigh</code>: spike times over 30 s in the high ambient light condition</p>

<p>We can take these two variables from <code class="highlighter-rouge">data</code> so that we can work with them directly.</p>

<div class="python-note">

  <p>Recall that the <code class="highlighter-rouge">loadmat()</code> function outputs a dictionary that contains the variables in the .mat file along with some additional information about the file.</p>

</div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SpikesLow</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'SpikesLow'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">SpikesHigh</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'SpikesHigh'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>Each variable is a single vector that gives a set of increasing spike times for the associated condition. The two vectors are of different sizes because the neuron fired a different number of spikes in each condition.</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> What is the size of the vector <code class="highlighter-rouge">SpikesLow</code>?

</p>

  <p>
  <br />
<strong>A.</strong> To answer this in Python, we use the command

    SpikesLow.shape

Python returns the answer
  <br />
    (750,)
  <br />
which reveals that <code class="highlighter-rouge">SpikesLow</code> is a vector with 750 elements (i.e., an array with 750 rows and 1 column). We could also have used <code class="highlighter-rouge">len(SpikesLow)</code> here since we are working with a vector (1-D) rather than a multidimensional array. Our collaborator who collected the data told us that each row holds a single spike time, and we continue to consider the implications of this statement.

</p>

</div>

<div class="question">

  <p><strong>Q.</strong> What is the size of the vector <code class="highlighter-rouge">SpikesHigh</code>?</p>

</div>

<p>Inspection of the sizes of the vectors <code class="highlighter-rouge">SpikesLow</code> and <code class="highlighter-rouge">SpikesHigh</code> reveals an important fact: the neuron fires more in the high-light condition. To make this observation more concrete, let’s compute the firing rate ($f$), defined mathematically as</p>

<script type="math/tex; mode=display">f = \frac{n}{T},
\tag{1}</script>

<p>where $n$ is the number of spikes over the time interval $T$.</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> What is the firing rate $f$ of the neuron recorded in the low ambient light condition?

</p>

  <p>
  <br />
<strong>A.</strong> To answer this question, we must first define two quantities of interest: $n$ and $T$. We consider here the entire duration of the recording, so $T = 30$ (our collaborator recorded the spiking activity for 30 s in each condition). During this interval, we found that the vector <code class="highlighter-rouge">SpikesLow</code> contains 750 spikes. With these two pieces of information, we may compute the firing rate.

</p>

</div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">T</span>

<span class="k">print</span><span class="p">(</span><span class="s">'f ='</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>f = 25.0

</code></pre></div></div>

<p>This tells us that the firing rate is 25 spikes per second, or 25 Hz.</p>

<div class="question">

  <p><strong>Q.</strong> Is this single number a good representation of the spike train data? What if the spiking changes dramatically throughout the recording?</p>

</div>

<div class="question">

  <p><strong>Q.</strong> What is the firing rate of the neuron recorded in the high ambient light condition?</p>

</div>

<p>These calculations allow us to compute a simple number representative of one aspect of the data: the firing rate over the entire duration of the recording. Do the two datasets exhibit a statistically significant change in the firing structure between conditions? Or, does the difference in firing rates lie within the range of expected fluctuations between any two trials of random spiking data? To answer these types of questions, we need to develop statistical methods that are appropriate for analyzing spike trains. Let’s look at the data more carefully and visualize the structure of the spiking in the low ambient light condition. Motivated by the results of the previous chapters, it may be tempting to visualize the spike train by simply plotting the <code class="highlighter-rouge">SpikesLow</code> variable,</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_22_0.png" alt="png" /></p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> What went wrong here? How do we interpret this plot?

</p>

  <p>
  <br />
<strong>A.</strong> These data are stored as a sequence of $N$ time stamps representing an increasing sequence of times at which the neuron spiked. When we run the plot command with only one input vector, we plot an index that runs from 1 to $n$ (the length of the vector) on the $x$-axis against the numerical values in that vector on the $y$-axis. Therefore the plot shows an increasing line where the $x$-axis represents the spike number and the $y$-axis represents the spike time. Notice that the vector ends at an $x$-axis value of 750, which corresponds to the length of the vector. Also, the values on the $y$-axis range from 0 to 30; these correspond to times starting near 0 s and ending near 30 s, as we expect for the 30 s recording. Although this plot is not immediately useful, the results are consistent with our expectations for the data.

</p>

</div>

<p>Instead of the data representation above, we would like to plot the spike train data as a set of points in a single row with $x$-coordinates that occur at the spike times. One way to produce such a plot is the following:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>  <span class="c"># Import the NumPy module</span>
<span class="n">plot</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">),</span> <span class="s">'.'</span><span class="p">)</span>  <span class="c"># Plot spikes as a row,</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>                                <span class="c"># ... display times (0, 5) s</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Time (s)'</span><span class="p">)</span>                          <span class="c"># ... label the x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>                                  <span class="c"># ... remove y-axis ticks</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_25_0.png" alt="png" /></p>

<div class="question">

  <p><strong>Q.</strong> The first line of code imports the NumPy module. What’s happening in the second line of this code?</p>

  <p><strong>A.</strong> The plot function receives three inputs. The first input defines the $x$-axis values for the plot, which here are the spike times. The second input is itself a function:</p>

  <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.ones_like(SpikesLow). 
</code></pre></div>  </div>

  <p>The function <code class="highlighter-rouge">ones_like</code> produces an array filled entirely with 1s that is the same dimensions as <code class="highlighter-rouge">SpikesLow</code>. The last input to <code class="highlighter-rouge">plot</code> instructs Python to display the data using the dot symbol. To summarize, we’re calling the <code class="highlighter-rouge">plot</code> command to display</p>

  <ul>
    <li>$x$-axis values: spike times in the low ambient light condition</li>
    <li>$y$-axis values: 1</li>
  </ul>

  <p>as blue dots. The next two commands set the range of the $x$-axis (in this case from 0 s to 5 s) and provide an $x$-axis label. The last command removes the $y$-axis tick marks since they don’t carry any real information in this case.</p>

</div>

<p>In the plot of the spike train above each spike time corresponds to a blue dot at a $y$-axis value of 1. The value on the $y$-axis is arbitrary. We could have chosen to use a $y$-axis value of 2 or -100 or 412. What matters is the $x$-axis, which indicates the time at which each spike occurs in the 5 s interval.</p>

<p>To compare the spiking in the low- and high-light conditions, we can plot both in the same figure:
<a id="fig:8.2b"></a></p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">),</span> <span class="s">'.'</span><span class="p">)</span>  <span class="c"># Plot the low-light condition spikes</span>
<span class="n">plot</span><span class="p">(</span><span class="n">SpikesHigh</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">SpikesHigh</span><span class="p">),</span> <span class="s">'.'</span><span class="p">)</span>  <span class="c"># ... and the high-light condition spikes </span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>              <span class="c"># Display times 0 to 5 s on the x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>              <span class="c"># ... and set the y-axis limits</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Time [s]'</span><span class="p">)</span>            <span class="c"># ... label the x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">({</span><span class="s">'Low'</span><span class="p">,</span> <span class="s">'High'</span><span class="p">})</span>   <span class="c"># ... show a legend</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_28_0.png" alt="png" /></p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> What’s happening in the fifth line of this code segment?

</p>

  <p>
  <br />
<strong>A.</strong> The fifth line of this code segment is similar to the second line, but it plots the data for the high ambient light condition. The first input to the <code class="highlighter-rouge">plot</code> function is the variable <code class="highlighter-rouge">SpikesHigh</code>. The second input to <code class="highlighter-rouge">plot</code> is a function; here we’re creating an array of 1s, this time with dimensions to match the variable <code class="highlighter-rouge">SpikesHigh</code>. Notice that we multiply this array by a scalar value of 2; this command acts to create a vector of 2s with the same dimensions as the vector <code class="highlighter-rouge">SpikesHigh</code>. The last input to <code class="highlighter-rouge">plot</code> indicates to display the data using another dot symbol. To summarize, here we’re calling the <code class="highlighter-rouge">plot</code> command to display

&lt;p&gt;
    &lt;ul&gt;
&lt;li&gt; x-axis values: spike times in the high ambient light condition &lt;li&gt; y-axis values: 2.
    &lt;/ul&gt;
</p>

  <p>&lt;/p&gt;</p>

</div>

<div class="question">

  <p><strong>Q.</strong> Explain why the following command fails to execute:</p>

  <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plot(SpikesLow, np.ones_like(SpikesHigh), '.')
</code></pre></div>  </div>

</div>

<p>With the data visualized in this way, we’re now able to ask an interesting question: What structure do you notice in the two spike trains <a href="#fig:8.2b">(figure)</a>? At first glance, your answer might be “not much.” Spikes occur fairly regularly throughout the 5 s interval under both conditions. Perhaps a careful visual inspection suggests there are fewer spikes in the low-light than in the high-light condition. But the spike times themselves do not seem to be directly comparable between these conditions. Often, when we examine data from a stimulus response experiment, we expect to see regions where spiking activity increases or decreases as a function of a changing stimulus. In this case, the stimulus is the ambient light level, which remains constant over the entire experiment. How else can we analyze these data and identify differences in the spiking activity (if any) between the two conditions?</p>

<div class="question">

  <p><strong>Q.</strong> Up to now, we’ve plotted a 5 s interval of data that begins at time 0 s. Through visual inspection, do you find similar conclusions for other 5 s intervals chosen from the data?</p>

</div>

<p><a href="#top">[Back to top]</a></p>

<p><a id="isi"></a></p>
<h3 id="examining-the-interspike-intervals">Examining the Interspike Intervals</h3>

<p>So far, we have examined the long-term structure of the spiking over multiple seconds. Let’s now focus on the short-term structure that occurs within a single second or less. Instead of plotting 5 s of spike train data, let’s plot an interval of 1 s:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">),</span> <span class="s">'.'</span><span class="p">)</span>  <span class="c"># Plot the low-light condition spikes</span>
<span class="n">plot</span><span class="p">(</span><span class="n">SpikesHigh</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">SpikesHigh</span><span class="p">),</span> <span class="s">'.'</span><span class="p">)</span>  <span class="c"># ... and the high-light condition spikes </span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>              <span class="c"># Display times 0 to 5 s on the x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>              <span class="c"># ... and set the y-axis limits</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Time [s]'</span><span class="p">)</span>            <span class="c"># ... label the x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">({</span><span class="s">'Low'</span><span class="p">,</span> <span class="s">'High'</span><span class="p">})</span>   <span class="c"># ... show a legend</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_35_0.png" alt="png" /></p>

<div class="question">

  <p><strong>Q.</strong> The code above was copied and pasted from the previous section with one minor update to change the time interval. What adjustments did we need to make to the code so that we see an interval of 1 s instead of 5 s?</p>

</div>

<p>Inspecting smaller time intervals, you might notice bursts of spikes that cluster near each other in time, interspersed with longer periods that contain less spiking. These patterns of bursts and quiescence look different between the low- and high-light stimuli. Visual inspection is an important tool, but we would like a quantitative result. How might we compare this fine temporal structure in the two conditions?</p>

<p>One approach to further characterizing the differences in spiking between the two conditions is to transform the data. One of the most useful transformations focuses on the waiting times between the spikes, or interspike intervals (ISIs), instead of the spike times themselves. We can compute the ISIs for the two conditions as follows:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ISIsLow</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">)</span>  <span class="c"># Compute ISIs in the low-light condition</span>
<span class="n">ISIsHigh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">SpikesHigh</span><span class="p">)</span>  <span class="c"># Compute ISIs in the high-light condition</span>
</code></pre></div></div>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> How do these commands represent the ISIs of the data?

</p>

  <p>
  <br />
<strong>A.</strong> Let’s focus on the first command, which defines the variable ISIsLow. Here, we use the function <code class="highlighter-rouge">diff()</code> with input <code class="highlighter-rouge">SpikesLow</code>. If you have not seen the command <code class="highlighter-rouge">diff()</code> before, look it up using <code class="highlighter-rouge">np.diff()?</code>. Briefly, the <code class="highlighter-rouge">diff()</code> command computes the difference between adjacent elements of the input. In this case, the vector <code class="highlighter-rouge">SpikesLow</code> represents the times at which spikes occur. Therefore, the difference between adjacent elements of <code class="highlighter-rouge">SpikesLow</code> produces the time interval or waiting time between successive spikes. To further explore the concept of an ISI, let’s write the spike times as a vector,

<script type="math/tex">v = {t_1, t_2, t_3, ..., T_N}, 
\tag{2}</script>

where $t_i$ is the time of the $i^{th}$ spike. The difference between the first two adjacent elements of $v$ is 

<script type="math/tex">t_2 - t_1
\tag{3}</script>

By convention, the <code class="highlighter-rouge">diff()</code> function subtracts the first element from the second. In words, this difference represents the time of the second spike ($t_2$) minus the time of the first spike ($t_1$), or the first interspike interval in the data. The difference between the next two adjacent elements of $v$ is

<script type="math/tex">t_3 - t_2
\tag{4}</script>

which is the second ISI, and so on. In this way, <code class="highlighter-rouge">diff()</code> converts the spike times in the vector <code class="highlighter-rouge">SpikesLow</code> into interspike intervals saved in the variable <code class="highlighter-rouge">ISIsLow</code>.

</p>

</div>

<div class="question">

  <p><strong>Q.</strong> Consider the variables <code class="highlighter-rouge">ISIsLow</code> and <code class="highlighter-rouge">ISIsHigh</code>. How do the sizes of these variables compare to the sizes of the corresponding spike trains <code class="highlighter-rouge">SpikesLow</code> and <code class="highlighter-rouge">SpikesHigh</code>, respectively? <em>Hint:</em> Given $N$ spikes, how many ISIs must occur?</p>

</div>

<p>The variables <code class="highlighter-rouge">ISIsLow</code> and <code class="highlighter-rouge">ISIsHigh</code> are vectors, and we can visualize these vectors using the same tools we’ve applied to visualize vectors in other scenarios. For example, we may simply plot these vectors:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_42_0.png" alt="png" /></p>

<p>The x-axis is the vector index, which ranges from 1 to the length of the vector ISIsLow. The y-axis is the value of the ISI at each index. We see that the ISI values range from small times (less than 0.05 s) to large times (over 0.4 s). In this way, the visualization provides some insight into the ISI values for the low-light condition.</p>

<div class="question">

  <p><strong>Q.</strong> Plot the ISI vector for the high-light condition and compare it to the ISI vector for the low-light condition. What similarities and differences do you notice in the ISIs from the two conditions?</p>

</div>

<div class="question">

  <p><strong>Q.</strong> What is the smallest ISI you expect to observe for a neuron? Would you be surprised to find an ISI of less than 1 second? of less than 1 millisecond? of less than 1 nanosecond?</p>

</div>

<p>Plots of the ISI vectors provide some information about the data (e.g., the approximate range of the ISI values), but there’s more insight to be gained. To that end, let’s now implement another approach to visualizing these types of data: the histogram. The idea of a histogram is to count the number of occurrences of each value in the data. In this case, we count the number of times we observe an ISI value in different bins of time. Let’s define the time bins for the histogram. Inspection of the ISI data for the low-light condition reveals values that range from near 0 s to over 0.4 s. Therefore, we choose the following time bins:</p>

<p>Bin 0 [0.00 0.01]</p>

<p>Bin 1 [0.01 0.02]</p>

<p>Bin 2 [0.02 0.03]</p>

<p>Bin 3 [0.03 0.04]</p>

<p>Bin 4 [0.04 0.05]</p>

<p>Bin 5 [0.05 0.06]</p>

<p>Bin 6 [0.06 0.07]</p>

<p>…</p>

<p>Bin N [0.49 0.50]</p>

<p>The bins begin at time 0 s and end at time 0.5 s, with a bin size of 0.01 s. The purpose of the histogram is to count the number of times the data values fall into each bin. Notice that we’ve chosen the range of bins to extend beyond the observed range of data ISIsLow; that’s fine, and we expect to count no values in the bins near 0.5 s. To further explore this counting process, let’s examine the first eight values of the data <code class="highlighter-rouge">ISIsLow</code>:</p>

<div class="python-note">

  <p>Recall that Python uses <em>zero-based indexing</em>. This means that elements in arrays are numbered starting with 0, which is why we labeled the first bin “Bin 0” instead of “Bin 1”.</p>

</div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ISIsLow</span><span class="p">[:</span><span class="mi">8</span><span class="p">]</span>
</code></pre></div></div>

<div class="output output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.04098354, 0.02902169, 0.00746714, 0.05205904, 0.05553601,
       0.06204051, 0.02267623, 0.02132764])
</code></pre></div></div>

<p>We see that the first value of ISIsLow is approximately 0.0410. In which bin does this value belong? Examining the list of bins, we find that <code class="highlighter-rouge">ISIsLow[0]</code> lies in bin 4, so we increment the number of counts in bin 4 by 1. The second value of the vector <code class="highlighter-rouge">ISIsLow</code> (<code class="highlighter-rouge">ISIsLow[1]</code> $\approx$ 0.0290) lies in bin 2, so we increment the number of counts in bin 2 by 1. The third value, <code class="highlighter-rouge">ISIsLow[2]</code> $\approx$ 0.0075, lies in bin 0, so we increment the number of counts in bin 0 by 1. The fourth value, <code class="highlighter-rouge">ISIsLow[4]</code> $\approx$ 0.0521, lies in bin 5, so we increment the number of counts in bin 5 by 1. And the fifth value, <code class="highlighter-rouge">ISIsLow[4]</code> $\approx$ 0.0555, also lies in bin 5, so we again increment the number of counts in bin 5 by 1.</p>

<div class="question">

  <p><strong>Q.</strong> At this point, for the first five entries of the vector <code class="highlighter-rouge">ISIsLow</code>, how many counts are there in each bin?</p>

  <p><strong>A.</strong> We find for the first five ISIs in the low-light condition, zero counts in all bins except</p>

  <p>Bin 0: 1 count 
Bin 2: 1 count 
Bin 4: 1 count 
Bin 5: 2 counts</p>

  <p>Notice that only four bins have counts and that bin 5 has two counts; for the first five ISIs in the low-light condition, we observe two ISIs in the interval (0.05, 0.06).</p>

</div>

<div class="question">

  <p><strong>Q.</strong> Repeat the binning procedure for the first eight ISIs observed in the high-light condition. Which bins have 1 or more counts for the first eight ISI values? What is the number of counts in each of these bins?</p>

</div>

<div class="question">

  <p><strong>Q.</strong> Consider the first eight ISI values. In the previous question, you placed each of these values in a bin. Sum the counts across all bins. What do you find? Think about the value you compute; does the result make sense?</p>

</div>

<p>Of course, we’re free to choose any interval of bins for the histogram. In the preceding examples, we chose a bin size of 0.01 s = 10 ms. Based on our understanding of a neuron, we might instead choose to examine a smaller bin size, 0.001 s = 1 ms. Let’s do so now, and examine the histogram of all ISIs for the low-light condition. Of course, with enough patience, we could examine by hand each ISI value from the low-light condition and place each value in the correct 1 ms bin. However, this process would be time consuming and extremely error prone. Instead, the process of binning the ISI data is better done in Python. To create a histogram of all the ISI data in the low-light condition is straightforward:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>  <span class="c"># Define the bins for the histogram</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>        <span class="c"># Plot the histogram of the ISI data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">])</span>            <span class="c"># ... focus on ISIs from 0 to 150 ms</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'ISI [s]'</span><span class="p">)</span>              <span class="c"># ... label the x-axis</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Counts'</span><span class="p">)</span>               <span class="c"># ... and the y-axis</span>
<span class="n">title</span><span class="p">(</span><span class="s">'Low-light'</span><span class="p">)</span>             <span class="c"># ... give the plot a title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_54_0.png" alt="png" /></p>

<p>In the first line of this code segment, we define the bins. These bins start at time 0 s and end at time 0.499 s, and the size of each bin is 0.001 s. We then call the function <code class="highlighter-rouge">hist()</code> with two inputs: the first input is the variable we’d like to examine (here, <code class="highlighter-rouge">ISIsLow</code>, the ISIs in the low-light condition), and the second input is the bins. The function <code class="highlighter-rouge">hist()</code> computes the histogram and displays the result. By setting the $x$-axis limit with <code class="highlighter-rouge">xlim()</code>, we’ve chosen to examine the ISI values from 0 ms to 150 ms. We’ve also labeled the axes in the resulting figure. Notice that the $x$-axis indicates the binned ISI intervals, while the $y$-axis indicates the number of counts in each bin.</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> Repeat this procedure to create a histogram of the ISI data in the high-light condition. Use the same bins we applied in the low-light condition (i.e., a bin size of 1 ms, extending from 0 s to 0.5 s). What do you find? <em>Hint:</em> Compare your answer to plot above.

</p>

</div>

<p>Let’s visualize the distributions of ISIs in both conditions next to each other: <a id="fig:8-5"></a></p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>        <span class="c"># Plot the histogram of the low-light ISI data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">])</span>            <span class="c"># ... focus on ISIs from 0 to 150 ms</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Counts'</span><span class="p">)</span>               <span class="c"># ... label the y-axis</span>
<span class="n">title</span><span class="p">(</span><span class="s">'Low-light'</span><span class="p">)</span>             <span class="c"># ... give the plot a title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ISIsHigh</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>       <span class="c"># Plot the histogram of the high-light ISI data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">])</span>            <span class="c"># ... focus on ISIs from 0 to 150 ms</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'ISI [s]'</span><span class="p">)</span>              <span class="c"># ... label the x-axis</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Counts'</span><span class="p">)</span>               <span class="c"># ... and the y-axis</span>
<span class="n">title</span><span class="p">(</span><span class="s">'High-light'</span><span class="p">)</span>            <span class="c"># ... give the plot a title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_58_0.png" alt="png" /></p>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_58_1.png" alt="png" /></p>

<div class="question">

  <p><strong>Q.</strong> Describe the features of the two histograms. What features of the ISI distributions are similar for the two conditions? What features are most strikingly different?</p>

</div>

<p>In the ISI histogram of the low-light condition data, very few counts occur at small ISI values (near 0 ms) and high ISI values (beyond approximately 100 ms). Instead, the distribution of counts is broadly peaked in the approximate interval 5–20 ms.</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> What does the ISI distribution reveal about the spiking activity in the low-light condition? From this ISI distribution, could you sketch a spike train consistent with these data?

</p>

  <p>
  <br />
<strong>A.</strong> We conclude from the ISI distribution that many spikes are separated by time intervals 5–20 ms. So, we might be tempted to imagine near-periodic spiking with a period 5–20 ms:

<a id="fig:8.6a"></a>

<img src="imgs/8-6a.png" alt="Cartoon representation of spiking activity." title="Near-periodic spiking, with period 5&mdash;20 ms." />

However, the histogram contains additional structure beyond the single broad peak. Indeed, the histogram has a long tail, with counts extending up to 150 ms. Therefore, the intervals between spikes are varied. We often see ISIs in the 5–20 ms interval, but we also find much longer ISIs (e.g., from 50 to 150 ms). The structure of the ISI histogram is consistent with bursting activity, which consists of intervals of rapid spiking interspersed with quiescence:

<a id="fig:8.6b"></a>

<img src="imgs/8-6b.png" alt="Cartoon representation of spiking activity." title="Bursting activity, with both short and long intervals between spikes." />

The intervals of rapid spiking produce many shorter ISIs, and the longer intervals produce (typically fewer) longer ISIs. We may conceptualize a bursting neuron as having two time scales: fast and slow. From the shape of the histogram of the low-light condition, we conclude that the spike train data in the low-light condition are more consistent with bursting activity than with periodic, metronome-like spiking activity.
  <br />
</p>

</div>

<div class="question">

  <p><strong>Q.</strong> What does the ISI distribution reveal about the spiking activity in the high-light condition? From this ISI distribution, could you sketch a spike train consistent with these data?</p>

</div>

<p>Let’s now consider the two ISI histograms representing the spiking activity from the two conditions. We note that both histograms show no ISI values below 0 ms. This is to be expected; the intervals between spikes cannot be negative. Both histograms also show broad peaks at 5–20 ms, indicating that a large number of short ISIs appear in the spike trains. In addition, both histograms possess long tails (i.e., counts of ISIs at larger bins, beyond 50 ms). A reasonable conclusion is that both neurons exhibit bursting activity, intervals of rapid spiking separated by periods of quiescence. However, a prominent difference exists between the ISI histograms in the two conditions. In the high-light condition, the proportion of small ISIs is much larger. The visualizations of the ISI histograms provide additional evidence of the similarities and differences in the spiking activity from the two conditions. We continue to investigate these two datasets—and build our scientific conclusions—in the next sections.</p>

<div class="question">

  <p><strong>Q.</strong> We claim that the neuron exhibits bursting activity in both histograms. But clearly the two ISI histograms are different. How does the bursting activity differ in the low- and high-light conditions? <em>Hint:</em> Consider the impact of the large proportion of small ISIs in the high-light condition.</p>

</div>

<div class="question">

  <p><strong>Q.</strong> So far, we’ve investigated two bin sizes: 10 ms and 1 ms. How do the shapes of the histograms above depend on the bin size used? Was 1 ms a good choice in this case? Why, or why not?</p>

</div>

<p><a href="#top">Back to top</a>
<a id="bsi"></a></p>
<h3 id="examining-binned-spike-increments">Examining Binned Spike Increments</h3>

<p>Another common approach to analyzing spiking data is to discretize time into bins of fixed width and count the number of events that occur in each time bin. The sequence of spike counts across all the bins is sometimes called the increment process for the spike train. When the time bins are sufficiently small, say, 1 ms for typical spike train data, the resulting increment process is just a sequence of zeros and ones. In this case, the time bins are so small that the probability of more than one spike occurring in each bin is zero or negligibly small.<sup><abbr title="The biophysical mechanisms that produce a spike support this statement. After generating a spike, the neuron experiences a refractory period typically lasting a few milliseconds, in which generating a subsequent spike is very unlikely.">note</abbr></sup> Each tiny time bin then contains a spike (and we assign that bin a value of 1) or does not (and we assign that bin a value of 0). This idea of representing the spike train as a sequence of zeros and ones for small bin increments will be important when we build statistical models of the spike trains (<strong>TBD: link to book ch9</strong>). In this section, we compute the increment process with multiple bin sizes in order to characterize the amount of variability in the spiking data and to examine temporal dependencies between spikes.</p>

<p>Let’s bin the spike train data of the low-light condition into time bins of size 50 ms. To do so, we make use of the function <code class="highlighter-rouge">histogram()</code>:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">time_bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>                    <span class="c"># Define the time bins</span>
<span class="n">IncrementsLow50</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">,</span> <span class="n">time_bins</span><span class="p">)</span>  <span class="c"># ... and compute a histogram of the data</span>
<span class="n">plot</span><span class="p">(</span><span class="n">time_bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">IncrementsLow50</span><span class="p">,</span> <span class="s">'.'</span><span class="p">)</span>               <span class="c"># Plot the resulting counts over time</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Time [s]'</span><span class="p">)</span>                                       <span class="c"># ... with axes labeled</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Number of Spikes'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_67_0.png" alt="png" /></p>

<p>Notice that, in this case, we use the function <code class="highlighter-rouge">histogram()</code> in a new way. Instead of simply generating a plot, we call the function with the output variable <code class="highlighter-rouge">IncrementsLow50</code>. This variable <code class="highlighter-rouge">IncrementsLow50</code> is a vector containing the number of counts (i.e., the number of spikes) in each 50 ms increment. The time bins (<code class="highlighter-rouge">time_bins</code>, the second input to the function <code class="highlighter-rouge">histogram</code>) is a vector containing the bin locations in time. In this case, the time bins start at time 0 s and end at time 30 s, with 0.05 s between bins. The variable name <code class="highlighter-rouge">IncrementsLow50</code> is quite descriptive. It reminds us that the variable represents the increments process in the low-light condition with a time bin of 50 ms.</p>

<div class="python-note">

  <p>A descriptive choice of variable name is often useful.</p>

</div>

<div class="question">

  <p><strong>Q.</strong> What can you say about the spike train data based on the increment process that we just plotted? Approximately how often do you observe a 50 ms increment with zero spikes? With four spikes?</p>

</div>

<p>One question that arises quite often is how variable these binned counts are. To illustrate this variability, let’s consider two scenarios. In the first, consider a neuron that fires perfectly regularly, like a metronome. In this case, we expect the number of spikes in each time bin to be nearly identical. On the other hand, consider the scenario of a neuron that fires in irregular bursts. In this case, we expect much more variability in the number of spikes in each time bin, depending on whether a time bin contained a burst of spikes or a quiet period. To characterize this variability, a standard measure to compute is the sample Fano factor (FF). It’s easy to define the Fano factor: FF is the sample variance of the increment process divided by the sample mean of the increment process. The implementation of FF in Python is also relatively simple:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">FF50Low</span> <span class="o">=</span> <span class="n">IncrementsLow50</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">/</span> <span class="n">IncrementsLow50</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'FF50Low ='</span><span class="p">,</span> <span class="n">FF50Low</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FF50Low = 0.7164927285225824

</code></pre></div></div>

<div class="question">

  <p><strong>Q.</strong> How do we interpret this FF value?</p>

</div>

<p>To answer that question, we need to introduce the concept of a Poisson process. A Poisson process is a model for a spiking process for which each spike occurrence is independent of every other spike occurrence. In other words, the probability of a neuron spiking at any instant does not depend on when the neuron fired (or did not fire) previously. A useful way to conceptualize this process is as a coin flip. For example, consider the following outcome of 20 coin flips:</p>

<p>HTHTTTHTTTTTHHHHHHTH</p>

<p>where H indicates heads, and T indicates tails.</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> Based on your intuitive knowledge of a coin flip, does the result of a chosen coin flip depend on any other coin flip?

</p>

  <p>
  <br />
<strong>A.</strong> No. Consider, for example, the fifth coin flip. In the example outcome, the fifth coin flip resulted in T (tails). Does this result depend on the previous coin flip? on the next coin flip? on the first coin flip? on a future one-hundredth coin flip? In all cases, intuition suggests that it does not. Each coin flip is independent of every other coin flip. That’s the assumption we make in assuming a Poisson process as a model for spiking activity: each spike occurrence is independent of every other spike occurrence.

</p>

</div>

<p>The Poisson process is rarely an accurate model for spike train data. Our biological knowl- edge reveals that the occurrence of a spike does depend on the occurrence of previous spikes (e.g., because of the refractory period of a neuron, we do not expect a spike to occur immediately after another spike). However, the Poisson process has many nice theoretical properties, that make it a good model against which to compare the data. For example, for any Poisson process, the number of spikes in any time interval has a Poisson probability distribution for which the theoretical variance and mean are equal (see the <a href="#appendix">appendix</a> at the end of the chapter).</p>

<div class="math-note">

  <p>The theoretical Fano factor for a Poisson process is exactly equal to 1.</p>

</div>

<p>When measuring the variability of the increments of a spike train, we typically compare it to the variability of a Poisson process. If we compute a Fano factor well below the value 1 for a particular set of increments, this suggests that the spiking is more regular than a Poisson process for the time scale at which the increments were binned. In this case, spiking activity in the past is influencing the neuron to spike in a more predictable manner in subsequent bins. If we compute a Fano factor well above the value 1, this suggests that the spiking is more variable than a Poisson process for the time scale at which the increments were binned.</p>

<p>For the 50 ms binned spikes in the low-light condition, we obtained a sample Fano factor value of 0.72, well below 1. We might therefore conclude that the spiking data in the low-light condition are more regular (i.e., more like a metronome) than we expect for a Poisson process.</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> What can we conclude about the variability of the counts for the spiking data in the high-light condition?

</p>

  <p>
  <br />
<strong>A.</strong> Repeating the analysis, we find that the sample Fano factor in the high-light condition is 1.78, well above 1. We might therefore conclude that the spiking data in the high-light condition are less regular than we expect for a Poisson process.

</p>

</div>

<h4 id="does-the-observed-fano-factor-differ-from-1">Does the observed Fano factor differ from 1?</h4>

<p>The preceding results are somewhat unsatisfying. We claimed that in the low-light condition, the calculated Fano factor of 0.72 was well below 1. What if, instead, we calculated a Fano factor of 0.8; is that well below 1? Is a Fano factor of 0.9 well below 1? These questions highlight an important issue when drawing a conclusion from a Fano factor calculation: How far above or below the value of 1 does the calculated Fano factor have to be before we are confident that there is really a statistically significant difference in the variability from a Poisson process? After all, even if we had spiking from a true Poisson process, from one experiment to the next we would expect to find different values for the increments, and values for the sample Fano factor that fluctuate slightly above and below 1. Fortunately, a bit of statistical theory can help us out. It can be shown that the distribution of Fano factors that we might compute from a Poisson process follows a gamma distribution with shape parameter $(N - 1)/2$ and scale parameter
$2/(N - 1)$, where $N$ is the number of time bins used in the Fano factor calculation [<a href="https://www.ncbi.nlm.nih.gov/pubmed/20416340">Eden &amp; Kramer, 2010</a>].</p>

<div class="question">

  <p><strong>Q.</strong> What is the correct value of $N$ for the increment process computed above and saved in the variable IncrementsLow50?</p>

  <p><strong>A.</strong> We find in Python that</p>

  <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>IncrementsLow50.shape
</code></pre></div>  </div>

  <p>is 600. Therefore, N = 600. This makes sense; 600 time bins of 50 ms duration fit between 0 s and 30 s.</p>

</div>

<p>With the value of $N = 600$ now determined, let’s plot the gamma distribution and investigate its shape:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">gamma</span>   <span class="c"># Import the gamma object from the SciPy stats toolbox</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">IncrementsLow50</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>   <span class="c"># Determine number of time bins.</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>             <span class="c"># Define the shape parameter of the gamma function</span>
<span class="n">scale</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>             <span class="c"># ... and the scale parameter </span>
<span class="n">FF</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span> <span class="c"># Define possible FF values</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">FF</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> 
              <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>      <span class="c"># ... compute gamma distribution,</span>
<span class="n">plot</span><span class="p">(</span><span class="n">FF</span><span class="p">,</span><span class="n">Y</span><span class="p">);</span>                     <span class="c"># ... and plot it</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Fano Factor'</span><span class="p">)</span>           <span class="c"># ... with axes labeled</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Probability Density'</span><span class="p">)</span>  
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_83_0.png" alt="png" /></p>

<p>Notice that we’re evaluating the function <code class="highlighter-rouge">gamma.pdf()</code>, which returns as output the gamma probability density function. We provide three inputs to this function. The first specifies a range of Fano factors to investigate (here we choose Fano factors ranging from 0.5 to 1.5; we choose a large number of Fano factor values to make a smooth plot of the gamma distribution). The second and third inputs to the function specify the shape and scale parameters of the gamma distribution.</p>

<p>When $N$ is large, as it is here, the gamma distribution looks like a normal distribution (i.e., like a bell-shaped curve). We can use this distribution to construct an interval where we would expect the Fano factor to lie if the data were generated by a Poisson process. More specifically, if the data were generated by a Poisson process, then we would expect the Fano factor to lie in the 95% confidence interval around the value of 1. Let’s construct this 95% confidence interval:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gamma</span><span class="o">.</span><span class="n">ppf</span><span class="p">([</span><span class="o">.</span><span class="mo">025</span><span class="p">,</span> <span class="o">.</span><span class="mi">975</span><span class="p">],</span> <span class="n">shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.88985257, 1.11648138])
</code></pre></div></div>

<p>In this case, we use the function <code class="highlighter-rouge">gamma.ppf()</code> to compute the gamma inverse cumulative distribution function (or percentiles). The first input specifies the boundaries of the probabilities of interest, which range from 2.5% to 97.5%, to encompass 95% of the probability mass around the value of 1. The next two inputs specify the shape and scale parameters of the gamma distribution. Evaluating this line of code, we find the interval (0.890, 1.116). Therefore, if we observed a true Poisson process with N = 600 bins and computed the Fano factor, we would not be surprised to find values between 0.890 and 1.116. However, the observed Fano factor for an increment process with 50 ms bins in the low-light condition is well outside of this range; this result suggests that it is very unlikely that these data were generated by a Poisson process. Instead, it’s more likely that these data were generated by a process with less variability in the low-light condition (FF = 0.72).</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> Consider the Fano factor for the high-light condition. Do you believe these data were generated by a Poisson process? Why, or why not?

</p>

  <p>
  <br />
<strong>A.</strong> We found for the high-light condition a Fano factor of 1.78 (again using an increment process with 50 ms bins). This calculated value is well above the interval expected for a Poisson process. We therefore reject the hypothesis that these data were generated by a Poisson process. Instead, we observe more variability in the high-light condition than expected for a Poisson process (with $N = 600$).

</p>

</div>

<div class="question">

  <p><strong>Q.</strong> How do the results for the Fano factor change in each condition with different choices for the bin size of the increment process (e.g., 25 ms, 100 ms, 500 ms)? Note that by changing the bin size, you also change $N$. <em>Challenge</em>: See if you can write a function that takes the data and desired binwidth as input and returns the Fano factor and 95% confidence interval.</p>

</div>

<p><a href="#top">Back to top</a>
<a id="autocorrelations"></a></p>
<h3 id="computing-autocorrelations-for-the-increments">Computing Autocorrelations for the Increments</h3>

<p>Another way to characterize the history dependence structure of a spike train is with the <em>autocorrelation</em> function of the increments. A correlation coefficient describes the degree of linear dependence between any two variables. The value of the correlation ranges from -1 to 1. A correlation value of -1 indicates a perfect linear relation between the two variables with a negative slope. A value of 0 indicates no linear relation between the two variables. And a value of 1 indicates a perfect linear relation between the two variables with a positive slope. Any other value indicates that one variable can be predicted using a linear function of the other, but that prediction will be imperfect; the closer the value is to $\pm$1, the better the prediction will be. The sign of the coefficient indicates the slope of the linear relation. The following figure shows scatterplots for a variety of possible relations between two variables, and the values of the correlation coefficients.</p>

<p><img src="imgs/8-9.png" alt="Correlation values for example relations between two variables." title="Variables are plotted, one against the other, on the x-axis and y-axis. Numbers indicate values of the correlation between the two variables." /></p>

<p>Mathematically, the formula for the sample autocorrelation at a lag $L$ is
<a id="eq:5"></a></p>

<script type="math/tex; mode=display">\rho_{xx}[L] = \frac{
\sum_{i=1}^{N - L}
(x_i - \overline x)(x_{i+L} - \overline x)
}
{
\sum_{i=1}^{N}(x_i - \overline x)^2
}
\tag{5}</script>

<p>where $x_i$ is the $i^{th}$ data point, and $\overline x$ is the sample mean of the data over index $i$.</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> This formula is rather complicated, so let’s consider a simple case: $L = 0$. What is $\rho_{xx}[0]$ at lag 0?

</p>

  <p>
  <br />
<strong>A.</strong> To answer this, let’s substitute $L=0$ into the mathematical expression fo rthe autcorrelation,


\begin{eqnarray}
\rho_{xx}[0] &amp;=&amp; 
\bigg ( \sum_{i=1}^{N-0} (x_i - \overline x)(x_{i+0} - \overline x) \bigg ) 
\bigg / 
\bigg ( \sum_{i=1}^N (x_i - \overline x)^2 \bigg ) 
<br />
&amp;=&amp;
\bigg ( \sum_{i=1}^{N} (x_i - \overline x)(x_{i} - \overline x) \bigg ) 
\bigg / 
\bigg ( \sum_{i=1}^N (x_i - \overline x)^2 \bigg ) 
<br />
&amp;=&amp;
\bigg ( \sum_{i=1}^N (x_i - \overline x)^2 \bigg ) 
\bigg / 
\bigg ( \sum_{i=1}^N (x_i - \overline x)^2 \bigg ) 
<br />
&amp;=&amp; 1.
\end{eqnarray}

At $L=0$, the autocorrelation is by definition equal to 1. In words, an individual dataset $x$ is perfectly correlated with itself at zero lag.

</p>

</div>

<p>To gain some intuition for the autocorrelation, let’s consider the following relatively simple increment process:</p>

<p><a id="fig:8-10a"></a></p>

<p><img src="imgs/8-10a.png" alt="Example data for visual inspection of autocorrelation of an increment process." title="Data x as a function of time (arbitrary units)." /></p>

<p>Our goal is to understand the autocorrelation of these data, labeled $x$. From visual inspection of the figure, we conclude that the data $x$ have mean 0, so that $\overline x = 0$; note that the data appear to oscillate between equal and opposite values over time. For simplicity, let’s focus on the numerator of equation (<a href="#eq:5" class="thumb">5<span><img src="imgs/eq5.png" /></span></a>). At lag zero (i.e., $L = 0$), the numerator of (<a href="#eq:5" class="thumb">5<span><img src="imgs/eq5.png" /></span></a>) tells us to multiply the data by themselves at each time index, and then sum the product. To visualize this multiplication and sum, consider multiplying the data in figure above by the following data at each index in time and then summing the result.</p>

<p><a id="fig:8-10b"></a></p>

<p><img src="imgs/8-10b.png" alt="Example data for visual inspection of autocorrelation of an increment process." title="Data x at lag L=0." /></p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> What is the value of this sum?

</p>

  <p>
  <br />
<strong>A.</strong> We can’t answer this question without knowing the values of $x$. However, we can deduce the sign and relative size of the sum. At a time index where $x$ is positive, the product is positive; and at a time index where $x$ is negative, the product is still positive. So, the product at each time index will always be positive or zero. Therefore, we expect the sum to be a positive number. And if we add many terms, we expect this sum to be large. So, we may conclude that the value of this sum will be a large, positive number. Note that this number is the numerator of the autocorrelation. At $L = 0$, the numerator equals the denominator in (<a href="#eq:5" class="fig">5<span><img src="imgs/eq5.png" /></span></a>) and the autocorrelation at lag zero is 1.

</p>

</div>

<p>Let’s consider the numerator of the autocorrelation (<a href="#eq:5" class="fig">5<span><img src="imgs/eq5.png" /></span></a>) for a small positive lag (i.e., $L &gt; 0$). We can think of the small positive lag as shifting the data $x$ a little bit to the right:</p>

<p><a id="fig:8-10c"></a></p>

<p><img src="imgs/8-10a.png" alt="Example data for visual inspection of autocorrelation of an increment process." title="Data x." /></p>

<p><img src="imgs/8-10c.png" alt="Example data for visual inspection of autocorrelation of an increment process." title="Data x at lag L&gt;0." /></p>

<p>Now, to compute the numerator of the autocorrelation, we multiply $x$ by a shifted version of $x$ at each time index and then sum the result. In this case, we find some indices where the product of $x_i$ and $x_{i+L}$ is positive, and some indices where the product of $x_i$ and $x_{i+L}$ is negative; an example of an index where the product is negative is indicated above by the black circle. Visual comparison of figures two plots suggests that the product will be positive more often then negative. Therefore, we expect the sum of these products to still be positive, although not as large as we found for the $L = 0$ case. You may have noticed some points where the data do not overlap, at the far right and far left of the figure. At these locations, the product is zero.</p>

<div class="question">

  <p><strong>Q.</strong>  Consider the autocorrelation of the data $x$ at the positive lag $L_1$ shown here:</p>

  <p><img src="imgs/8-10d.png" alt="Example data for visual inspection of autocorrelation of an increment process." title="Data x at lag L1&gt;L." /></p>

  <p>What is the sign (positive or negative) and relative size of the numerator of the autocorrelation at this lag?</p>

</div>

<p>Let’s return to the spike train data of interest here, recorded in the low- and high-light conditions. For the corresponding spike train increments, the autocorrelation at a particular lag describes the relation between the spike counts in different bins separated by that lag. The autocorrelation function describes the autocorrelation across a range of lags over which we are interested. Given our visualizations of the ISI histograms (<a href="#fig:8-5" class="fig">figure<span><img src="imgs/8-5.png" /></span></a>), we might expect relations between spiking events to extend up to 200 ms.</p>

<p>Let’s compute the autocorrelation for increment processes deduced from the spike train data in the low-light condition. We compute the autocorrelation of the 50 ms increment process for lags ranging from 0 to 200 ms. We need only three lags to cover this range; lag 1 covers 50–100 ms, lag 2 covers 100–150 ms, and lag 3 covers 150–200 ms. We can define a function to compute the autocorrelation using the function <code class="highlighter-rouge">correlate()</code> from the NumPy module:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">autocorr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lags</span><span class="p">):</span>
    <span class="n">xcorr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">correlate</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="s">'full'</span><span class="p">)</span>  <span class="c"># Compute the autocorrelation</span>
    <span class="n">xcorr</span> <span class="o">=</span> <span class="n">xcorr</span><span class="p">[</span><span class="n">xcorr</span><span class="o">.</span><span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">/</span> <span class="n">xcorr</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>               <span class="c"># Convert to correlation coefficients</span>
    <span class="k">return</span> <span class="n">xcorr</span><span class="p">[:</span><span class="n">lags</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>                                     <span class="c"># Return only requested lags</span>
    
<span class="n">autocorr</span><span class="p">(</span><span class="n">IncrementsLow50</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1.        , 0.03894992, 0.07055464, 0.04431669])
</code></pre></div></div>

<p>Our new function <code class="highlighter-rouge">autocorr()</code> takes two inputs. The first input is the data for which we want to compute the autocorrelation, in this case, the increment process for the low-light condition with 50 ms time bins. The second input is the number of lags to compute. Within the function, we scale the raw autocorrelation so that we instead see correlation coefficients, which range from -1 to 1. Notice that we subtract from <code class="highlighter-rouge">IncrementsLow50</code> the mean of this variable before computing the autocorrelation. This command outputs a vector with four numerical values corresponding to the autocorrelation at lag indices 0 through 3. The autocorrelation at negative lags is mathematically identical to the autocorrelation at the equivalent positive lags, which is why we exclude them in the third line of the function definition.</p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> Examine the numerical values returned by <code class="highlighter-rouge">autocorr()</code>. What do you find?

</p>

  <p>
  <br />
<strong>A.</strong> As expected, the autocorrelation at zero lag is exactly equal to 1; the data matches itself at lag 0. At lag 1, corresponding to 50–100 ms, the autocorrelation value is 0.04. This positive correlation value indicates that when the number of spikes in one bin is higher than expected, the number of spikes in the next bin tends to be higher than expected. Similarly, when the number of spikes in one bin is lower than expected, the number of spikes in the next bin will also tend to be lower than expected. This effect is small, however, since the autocorrelation is near zero. At lag 2, corresponding to 100–150 ms, the autocorrelation value of 0.07 is again small and positive. At lag 3, corresponding to 150–200 ms, the autocorrelation value remains small and positive at a value of 0.04.

</p>

</div>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> How do we know whether these autocorrelation values are statistically significant?

</p>

  <p>
  <br />
<strong>A.</strong> This can be a difficult question when $N$ is small (less than 30), but for larger $N$ we can approximate a confidence interval about the correlation coefficient using a normal approximation with standard deviation $1/\sqrt N$. In this case, any correlation value exceeding $\pm2/\sqrt N$ is unlikely to be generated by chance and likely reflects real dependence structure. For the increment process considered here, $N = 599$, and the significance bound is $\pm 0.08$. We conclude that none of the lags has significant autocorrelation.

</p>

</div>

<p>If we are particularly interested in the fine-scale temporal dependence structure of the spikes, we would do better to compute the autocorrelation function for more finely binned intervals. To that end, let’s repeat the autocorrelation analysis for an increment process that uses 1 ms bins. We first compute a new increment process and then apply the <code class="highlighter-rouge">autocorr()</code> function to this process.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">time_bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>                    <span class="c"># Define the time bins</span>
<span class="n">IncrementsLow1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">SpikesLow</span><span class="p">,</span> <span class="n">time_bins</span><span class="p">)</span> <span class="c"># ... compute the histogram to create increment process</span>
<span class="n">ACFLow</span> <span class="o">=</span> <span class="n">autocorr</span><span class="p">(</span><span class="n">IncrementsLow1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>                 <span class="c"># ... and the autocorrelation</span>
</code></pre></div></div>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> What is the size of the output variable <code class="highlighter-rouge">ACFLow</code>? How does this size correspond to the lags?

</p>

  <p>
  <br />
<strong>A.</strong> <code class="highlighter-rouge">ACFLow</code> is a vector with dimensions 1 $\times$ 101. The 101 values correspond to lag indices 0 to 100 (or lag times 0 ms to 100 ms).

</p>

</div>

<p>In order to examie history dependence going back 100 ms, we need 100 lags (because each lag index corresponds to 1 ms). There are now too many values to examine them printed one by one at the command line, so instead we construct a plot fo the autocorrelation function with lag on the $x$-axis and correlation on the $y$-axis. Let’s also include in this figure two approximate significance lines at $\pm2/\sqrt N$.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">ACFLow</span><span class="p">,</span> <span class="s">'.'</span><span class="p">)</span>        <span class="c"># Plot autocorrelation vs lags,</span>
<span class="n">N1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">IncrementsLow1</span><span class="p">)</span>                    <span class="c"># ... compute the sample size</span>
<span class="n">sig</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">N1</span><span class="p">)</span>                       <span class="c"># ... and the significance level</span>
<span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="n">sig</span><span class="p">,</span> <span class="n">sig</span><span class="p">],</span> <span class="s">'r:'</span><span class="p">)</span>           <span class="c"># ... and plot the upper and lower significance lines</span>
<span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">sig</span><span class="p">,</span> <span class="o">-</span><span class="n">sig</span><span class="p">],</span> <span class="s">'r:'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>                          <span class="c"># ... set x-limits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>                         <span class="c"># ... and y-limits</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_105_0.png" alt="png" /></p>

<p>We see that, the two approximate significance lines at $\pm2/\sqrt N$ suggest significant negative correlation structure is present up to about $6$ ms. This reflects the refractory period of the neuron: if you observed a spike in the previous 6 ms, you are less likely to observe a spike in the next few milliseconds. Beyond this point, the values of the autocorrelation mostly remain between the two significance bounds.</p>

<div class="math-note">

  <p><strong>Alert!</strong> We are using these significance bounds here for exploratory purposes. We are not performing a rigorous statistical test for significance of the autocorrelation at every lag.</p>

</div>

<p>If we choose to perform a rigorous statistical test for the significance of the autocorrelation, we would face the multiple comparisons problem. Briefly, if we perform many independent tests, and each has a 5% chance of reaching significance by chance, then the probability that any of these tests reaches significance by chance can be very large. If we wanted to perform many tests, we would need to control for multiple comparisons by adjusting the significance level so that the probability of any test being significant by chance is small. In the plot above, the significance lines are not corrected for multiple comparisons. Therefore, we accept that some of the correlation values that exceed these bounds may occur by chance. However, it is still very unlikely that all the significant correlations we observed from 1 to 6 ms are occurring purely by chance.</p>

<p>Now that we’ve computed and interpreted the autocorrelation function for the low-light condition, let’s compare it to the autocorrelation in the high-light condition. We repeat our previous commands using the <code class="highlighter-rouge">SpikesHigh</code> data values and choosing a time bin of 1 ms:
&lt;a id=fig:8-12&gt;&lt;/a&gt;</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">IncrementsHigh1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">SpikesHigh</span><span class="p">,</span> <span class="n">time_bins</span><span class="p">)</span> <span class="c"># Compute the histogram to create increment process</span>
<span class="n">ACFHigh</span> <span class="o">=</span> <span class="n">autocorr</span><span class="p">(</span><span class="n">IncrementsHigh1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>                 <span class="c"># ... and the autocorrelation</span>
<span class="n">plot</span><span class="p">(</span><span class="n">ACFHigh</span><span class="p">,</span> <span class="s">'.'</span><span class="p">)</span>                                       <span class="c"># Plot the autocorrelation,</span>
<span class="n">sig</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">IncrementsHigh1</span><span class="p">))</span>                  <span class="c"># ... compute and plot the significance level,</span>
<span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="n">sig</span><span class="p">,</span> <span class="n">sig</span><span class="p">],</span> <span class="s">'r:'</span><span class="p">)</span>                               
<span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">sig</span><span class="p">,</span> <span class="o">-</span><span class="n">sig</span><span class="p">],</span> <span class="s">'r:'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>                                       <span class="c"># ... and set the plot limits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Time [ms]'</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Autocorrelation'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_109_0.png" alt="png" /></p>

<div class="question">

  <p>
  <br />
<strong>Q.</strong> Consider the autocorrelation of the spike train data in the high-light condition shown above. What do you observe? How do the autocorrelations differ in the two conditions?

</p>

  <p>
  <br />
<strong>A.</strong> We find in the high-light condition significant correlation structure going all the way out to about 50 ms. Once again, we see refractoriness reflected in the negative autocorrelation at a lag of 1 ms, but this lasts much less time than in the low-light condition. Instead, there is now significant positive correlation at intermediate lags (approximately 2–50 ms). This positive correlation at short time lags reflects the tendency of the neuron to fire in bursts with small ISIs in the high-light condition; after a spike, another spike is more likely to occur in the next 2–50 ms than in the subsequent 50–100 ms.

</p>

</div>

<p>Now that we’ve visualized the autocorrelations in the two light conditions, we can ask an important related question: Are the differences in the autocorrelations between these two conditions real? To answer this, we compute the difference in the autocorrelation functions between the low- and high-light conditions at every lag. If we assume that the firing in each condition is independent, the significance bounds for this difference can be computed by adding the variance of the autocorrelation from each condition. The standard deviation of the autocorrelation for the low-light condition is $1/\sqrt{N_1}$, so the variance of the auto- correlation for the low-light condition is $1/N_1$. For the high-light condition, the variance of the autocorrelation is $1/N_2$. We plot the differenced autocorrelations and the significance bounds:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">IncrementsHigh1</span><span class="p">)</span>
<span class="n">ACFDiff</span> <span class="o">=</span> <span class="n">ACFHigh</span> <span class="o">-</span> <span class="n">ACFLow</span>                    <span class="c"># Compute differences of autocorrelations</span>
<span class="n">plot</span><span class="p">(</span><span class="n">ACFDiff</span><span class="p">,</span> <span class="s">'.'</span><span class="p">)</span>                            <span class="c"># ... and plot them</span>
<span class="n">sd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">N1</span><span class="o">+</span><span class="mi">1</span><span class="o">/</span><span class="n">N2</span><span class="p">)</span>                      <span class="c"># ... with significance bounds</span>
<span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sd</span> <span class="o">*</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="s">'r:'</span><span class="p">)</span>
<span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sd</span> <span class="o">*</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="s">'r:'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>                            <span class="c"># Set the plot limits and label the axes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Time [ms]'</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Autocorrelation difference'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_112_0.png" alt="png" /></p>

<p>The results suggest significant differences in the autocorrelation between the two conditions at intermediate time lags (at approximately 2–50 ms). These are the same time lags we identified with bursting activity in the high-light condition. This suggests that the neuron fires with more intermediate ISIs in the bursting range in the high-light condition.</p>

<p><a href="#top">Back to top</a>
&lt;a id=acISI&gt;&lt;/a&gt;</p>
<h3 id="computing-autocorrelations-of-the-isis">Computing Autocorrelations of the ISIs</h3>

<p>The autocorrelation of the increments indicates the amount of time for which there are dependencies in the spiking data. In the high-light condition, we found large correlation values extending out to approximately 50 ms. This could be a consequence of the influence of patterns of many spikes with shorter ISIs or of single spikes with longer ISIs. We can distinguish between these possibilities by looking at the autocorrelation of the sequence of ISIs. In this case, the lag represents the number of spikes in the past rather than the amount of time in the past. If the dependence is only due to the last spike, we expect the ISIs to be uncorrelated at any nonzero lag. This would necessarily be true for data from a Poisson process. If we see correlation between ISIs, this suggests that the data do not come from a Poisson process and that the past spiking has an influence over multiple spikes. To investigate this, let’s compute the autocorrelation of the sequence of ISIs for the low-light condition:
&lt;a id=fig:8-14&gt;&lt;/a&gt;</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Compute and plot the autocorrelation of the low-light ISIs,</span>
<span class="n">ISI_ACF_Low</span> <span class="o">=</span> <span class="n">autocorr</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">ISI_ACF_Low</span><span class="p">,</span> <span class="s">'.'</span><span class="p">)</span>
<span class="c"># ... with upper and lower significance lines,</span>
<span class="n">N3</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>
<span class="n">sd</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">N3</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sd</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">ISI_ACF_Low</span><span class="p">),</span> <span class="s">'r:'</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sd</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">ISI_ACF_Low</span><span class="p">),</span> <span class="s">'r:'</span><span class="p">)</span>
<span class="c"># Set plot limits and label axes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">])</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Lags'</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Autocorrelation'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_115_0.png" alt="png" /></p>

<p>Notice that we include confidence bounds determined by the size of the sequence of interest (in this case, the length of the ISIs).
We see that the autocorrelation function has just a few isolated lags that are outside of the significance bounds. This could indicate a weak relation at particular lags or could be due to chance. Assuming the latter suggests that the data may come from a renewal process, a spiking process with independent ISIs for which the probability of a spike at any time only depends on the time of the most recent spike. One advantage of working with renewal processes is that it is fairly easy to write down and fit statistical models to the data. That is our next step.</p>

<div class="question">

  <p><strong>Q.</strong> Compute the correlation between ISIs for the data in the high-light condition. What do you find? Are these data consistent with a renewal process?</p>

</div>

<p><a href="#top">Back to top</a>
&lt;a id=models&gt;&lt;/a&gt;</p>
<h3 id="building-statistical-models-of-the-isis">Building Statistical Models of the ISIs</h3>

<p>In the previous sections, we constructed autocorrelation functions of the increment processes and autocorrelation functions of the sequences of ISIs. The former suggested dependence going back up to $\approx$ 50 ms (<a href="#fig:8-12" class="fig">figure<span><img src="imgs/8-12.png" /></span></a>), while the latter suggested that the spiking at any time depends only on the timing of the most recent spike (<a href="#fig:8-14" class="fig">figure<span><img src="imgs/8-14.png" /></span></a>). We now consider another powerful technique to understand these data: building a model. More specifically, we construct a <em>statistical model</em> of these data. This model captures important features of the data but does not consist of explicit biophysical components (an example of a biologically explicit model is the Hodgkin-Huxley equations [<a href="https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764">Hodgkin &amp; Huxley, 1952</a>]. The notion of a model can be confusing and is audience dependent, so we clarify here.</p>

<p>To construct a statistical model for these data we assume that the ISIs are independent samples from some unknown distribution. We typically posit some class of distributions from which the data might arise, and identify the one distribution in that class that maximizes the chance of observing the actual data.</p>

<p>What class of distributions should we use to build an ISI model? Previously, we discussed a Poisson process as a basic model for a spiking process, consistent with the conceptual idea of spikes as coin flips. Let’s fit a Poisson process with a constant firing rate to the observed data. In other words, we begin with a model where the number of spikes in any time bin is independent of all previous (and future) spiking and has a Poisson distribution with a fixed but unknown rate parameter $\lambda$. The probability $P$ of $k$ spikes in any time bin is given by the Poisson distribution,
<a id="eq:6"></a></p>

<script type="math/tex; mode=display">P(k) = \frac{
\lambda^k e^{-\lambda}
}{
k!
},
\tag{6}</script>

<p>where $k!$ is the factorial of $k$. Under this model, the distribution for the number of spikes in a bin is Poisson, but what is the distribution of the waiting time between spikes (i.e., what is the distribution of the ISIs)? It can be shown that for any Poisson process with constant firing rate the ISIs have an exponential distribution [<a href="https://www.springer.com/us/book/9781461496014">Kass, Eden &amp; Brown, 2014</a>]. Mathematically, the probability density function for any ISI taking on a value $x$ is
<a id="eq:7"></a></p>

<script type="math/tex; mode=display">f(x) = \lambda \exp(-\lambda x),
\tag{7}</script>

<p>where $\lambda$ is the rate parameter for the Poisson process.</p>

<div class="math-note">

  <p><strong>Alert!</strong> This is a common point of confusion. The increments of a Poisson process have a Poisson distribution, and the ISIs have an exponential distribution. The Poisson distribution takes on non-negative integer values {0,1,…,$\infty$}, which make it appropriate for counting the number of spikes in an interval. The Poisson distribu- tion does not make sense to describe the waiting time between spikes, since this typically takes on a continuous value in [0, $\infty$].</p>

</div>

<p>Our goal is to find a good value of $\lambda$ so that our statistical model (<a href="#eq:7" class="thumb">7<span><img src="imgs/eq7.png" /></span></a>) matches the observed ISI distributions. Let’s guess some values for $\lambda$, evaluate the model (<a href="#eq:7" class="thumb">7<span><img src="imgs/eq7.png" /></span></a>), and see how well the model matches the data. We plot the probability of observing ISI values in 1 ms bins for the low-light condition. This is similar to the ISI histogram we plotted previously except that the $y$-axis should represent probability instead of counts. To do so, we simply divide each count value by the total number of ISIs in the low-light condition:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>           <span class="c"># Define 1 ms bins for histogram,</span>
<span class="n">counts</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>  <span class="c"># ... compute histogram of the ISIs,</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>             <span class="c"># ... convert to probability,</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>                 <span class="c"># ... create figure and axes objects that we can reuse later,</span>
<span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">prob</span><span class="p">)</span>                 <span class="c"># ... and plot the probabilities,</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">])</span>                   <span class="c"># ... with fixed x-limits,</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'ISI [s]'</span><span class="p">)</span>                        <span class="c"># ... and axes labeled.</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Probability'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_121_0.png" alt="png" /></p>

<p>Now, on the same figure, let’s choose a value for $\lambda$ and plot the statistical model (<a href="#eq:7" class="thumb">7<span><img src="imgs/eq7.png" /></span></a>):</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span> <span class="o">=</span> <span class="mi">5</span>                                    <span class="c"># Choose a value for lambda,</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">l</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">l</span> <span class="o">*</span> <span class="n">bins</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>    <span class="c"># ... and create the model,</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="s">'g'</span><span class="p">)</span>                <span class="c"># ... and plot the model in green</span>
<span class="n">fig</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_123_0.png" alt="png" /></p>

<p>In this code, we have chosen $\lambda$ = 5 Hz and evaluated the statistical model at each time bin. We’ve also scaled the statistical model by a factor of 0.001 to match the 1 ms bin size, and plotted the model on top of the empirical ISI probability distribution.</p>

<div class="question">

  <p><strong>Q.</strong> Try using some different values of $\lambda$. What values of $\lambda$ appear to provide a good fit to the empirical distribution of ISI values?</p>

</div>

<p>The process of guessing values of $\lambda$ and comparing the model (<a href="#eq:7" class="thumb">7<span><img src="imgs/eq7.png" /></span></a>) to the empirical ISI distribution is not satisfying. How do we identify the parameter $\lambda$ that best fits the observed ISI distribution? We now consider a procedure to do so. Our goal is to find the value of $\lambda$ that maximizes the likelihood of the data given the statistical model (<a href="#eq:7" class="thumb">7<span><img src="imgs/eq7.png" /></span></a>); this value of $\lambda$ will be the best fit of the model to the data. To implement this procedure, let’s consider the probability density of observing a sequence of ISIs, $x_1, x_2, …, x_n$. If we assume that the ISIs are independent, then the probability density is
<a id="eq:8"></a></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
f(x_1, x_2, ..., x_n) &=& f(x_1) f(x_2) ... f(x_n) \\ 
&=& \lambda \exp(-\lambda x_1) \lambda \exp(-\lambda x_2) ... \lambda \exp(-\lambda x_n) \\
&=& \lambda ^n \exp(-\lambda \sum_{i=1}^n x_i).
\end{eqnarray}
\tag{8} %]]></script>

<p>We call this expression the joint probability distribution of the observed data. In the first equality, we separate the joint probability distribution $f(x_1,x_2,…,x_n)$ into a product of probability distributions of each event (i.e., $f(x_1)$, the probability of the first ISI equaling $x_1$ , multiplied by $f(x_2)$, the probability of the second ISI equaling $x_2$, multiplied by $f(x_3)$, the probability of the third ISI equaling $x_3$, and so on). This partitioning of the joint probability is valid here because we assume the ISIs are independent. In the second equality, we replace each probability distribution with the exponential distribution we expect for the ISIs of a Poisson process. In the last equality, we rewrite the expression as a single exponential. Notice that this last expression is a function of the unknown rate parameter, $\lambda$.</p>

<p>When considered as a function of the unknown parameters, the joint distribution of the data (<a href="#eq:8" class="thumb">8<span><img src="imgs/eq8.png" /></span></a>) is also called the <em>likelihood</em>. In this case, we write
<a id="eq:9"></a></p>

<script type="math/tex; mode=display">L(\lambda) = \lambda^n e^{-\lambda (x_1 + x_2 + ... + x_n)},
\tag{9}</script>

<p>to indicate that the likelihood $L$ is a function of $\lambda$. To understand what the likelihood function $L(\lambda)$ looks like , let’s plot it. We do so for the data from the low-ight condition, and consider a range of possible $\lambda$ values.
<a id="fig:8-16"></a></p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lambdas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>  <span class="c"># Range of lambda values.</span>
<span class="n">N3</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>        <span class="c"># Number of low-light ISIs observed.</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">lambdas</span> <span class="o">**</span> <span class="n">N3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lambdas</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">))</span>  <span class="c"># Compute the likelihood,</span>
<span class="n">plot</span><span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>         <span class="c"># ... and plot it</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'$</span><span class="err">\</span><span class="s">lambda$'</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Likelihood'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_127_0.png" alt="png" /></p>

<div class="question">

  <p><strong>Q.</strong> Consider the plot above. Does this answer seem okay?</p>

  <p><strong>A.</strong> Something went wrong here. The plot gives part of a line that rises toward $10^{-13}$ and is zero elsewhere. Why does this happen, and how can we fix it? To answer this, consider the first term in the likelihood function, $\lambda ^n$. In this case we are raising $\lambda$ to the power of <code class="highlighter-rouge">len(ISIsLow)</code> = 749. This is beyond the numerical precision limits of standard Python computations.</p>

</div>

<p>So, we can’t easily plot the likelihood directly. Instead, we plot the log of the likelihood. In this case, computing the log is useful because extremely large values are reduced to a more manageable range.
<a id="fig:8-17"></a></p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lambdas</span> <span class="o">=</span> <span class="n">lambdas</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c"># Update the lambda range to exclude 0.</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">N3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)</span> <span class="o">-</span> <span class="n">lambdas</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>  <span class="c"># Compute the log likelihood,</span>
<span class="n">plot</span><span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>       <span class="c"># ... and plot it.</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'$</span><span class="err">\</span><span class="s">lambda$'</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Log likelihood'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_130_0.png" alt="png" /></p>

<div class="question">

  <p><strong>Q.</strong> Consider the second line of code above. Does the definition for <code class="highlighter-rouge">l</code> correspond to $\log[L(\lambda)]$ (<a href="#eq:9" class="thumb">eq.<span><img src="imgs/eq9.png" /></span></a>)? <em>Hint</em>: It should. Remember $\log(x^a)=a \log x$, and $\log(e^b)=b$.</p>

</div>

<p>We see that the log likelihood is low for small $\lambda$, rises quickly as $\lambda$ increases, and then starts to fall off once $\lambda$ becomes larger than $\approx$ 25. The point $\lambda$ = 25, where the log likelihood is maximized, is called the maximum likelihood estimate of $\lambda$. We use the symbol $\hat\lambda$ to denote the maximum likelihood estimate of $\lambda$.</p>

<p>We observe that although the values of the likelihood go beyond the precision range in Python, the peak in the log likelihood stands out very clearly. Note that the likelihood (<a href="#fig:8-16" class="fig">figure<span><img src="imgs/8-16.png" /></span></a>) is maximized at the same point as the log likelihood (<a href="#fig:8-17" class="fig">figure<span><img src="imgs/8-17.png" /></span></a>). This is always true.</p>

<div class="question">

  <p><strong>Q.</strong> Can you explain why?</p>

</div>

<p>We could also have computed the maximum likelihood estimator theoretically, by differentiating the log likelihood with respect to $\lambda$, setting that equal to zero, and solving for $\lambda$. This gives $\frac{n}{\hat\lambda} - \sum_{i=1}^n x_i = 0$, which can be solved to find $\hat\lambda=(\sum_{i=1}^n x_i)^{-1} = 1 / \hat x = 25.0$ spikes/s. Remember that $x_i$ is the $i^{th}$ ISI value, so $\bar x$ is the average ISI value. This computation shows that the maximum likelihood estimate for the rate parameter of a Poisson process is just 1 divided by the average ISI value. For some statistical models, it is convenient to compute maximum likelihood estimates theoretically in this manner, but sometimes no closed-form solution exists. In these cases, we typically use numerical methods to solve for the maximum likelihood estimates.</p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> What is the maximum likelihood estimate for the Poisson rate parameter in the high-light condition?</p>

<p><strong>A.</strong> Repeating the analysis for the high-light condition, the maximum likelihood estimate for a Poisson rate parameter is $\hat \lambda = n(\sum{i=1}^nx_i)^{-1} = 1 / \bar x = 32.3$ spikes/s. The differenc ein the Poisson rate parameter of 32.3 - 25.0 = 7.3 spikes/s reflects the difference in the overall firing rate of the neuron between the low-and high-light conditions.</p>

<p>&lt;/div&gt;</p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Is the difference in the Poisson rate parameter between the low-and high-light conditions statistically significant?</p>

<p>&lt;/div&gt;</p>

<p>To address this last question, let’s use a bootstrap analysis (see <a href="../2.%20The%20Event-Related%20Potential/The%20Event-Related%20Potential.ipynb#bootstrap">chapter 2</a>). We combine all the ISIs from both conditions into one pool, sample many new datasets with replacement from that pool, and compare the actual difference in rate parameters to the distribution of differences across the samples.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Compute the observed difference in lambdas</span>
<span class="n">MLDiff</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">ISIsHigh</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">ISIsLow</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c"># Then, perform the bootstrap analysis.</span>
<span class="n">ISIs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">ISIsLow</span><span class="p">,</span> <span class="n">ISIsHigh</span><span class="p">])</span>  <span class="c"># Merge all ISIs.</span>
<span class="n">Nall</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIs</span><span class="p">)</span>  <span class="c"># Save length of all ISIs.</span>
<span class="n">Nlo</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>  <span class="c"># Save length for the low-light condition</span>
<span class="n">Nhi</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsHigh</span><span class="p">)</span>  <span class="c"># Save length high-light condition</span>

<span class="c"># Compute the difference in lambdas from resampled data</span>
<span class="n">sampDiff</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ISIs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">Nall</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Nhi</span><span class="p">)])</span> <span class="o">-</span>  <span class="c"># Resample the high-light ISIs and subtract</span>
           <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ISIs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">Nall</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Nlo</span><span class="p">)])</span>     <span class="c"># ... the resampled low-light ISIs</span>
           <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>                                    <span class="c"># ... 1000 times</span>

<span class="c"># Compare the bootstrap distribution to the empirical</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sampDiff</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>       <span class="c"># Plot resampled ISIs distribution</span>
<span class="n">plot</span><span class="p">([</span><span class="n">MLDiff</span><span class="p">,</span> <span class="n">MLDiff</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>  <span class="c"># ... and the empirical ISIs.</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Counts'</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Difference in rate (spikes/s)'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_138_0.png" alt="png" /></p>

<p>&lt;div class=math-note&gt;</p>

<p>There are more powerful tests we could use to compare the Poisson rate parameters. By more powerful, we mean that the tests are more likely to show a significant difference when one is actually present. However, the fact that the bootstrap test gives a significant result suggests that these more powerful tests would also be significant.</p>

<p>&lt;/div&gt;</p>

<p>But, is the Poisson model good? To answer this, let’s visualize the model fits compared to the data. There are a number of ways to do this. We start by comparing the expected proportion of ISIs for a Poisson process to the ISI histograms we actually observe in each condition. Let’s do so first for the low-light condition:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>            <span class="c"># Define 1 ms bins for histogram</span>
<span class="n">counts</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>   <span class="c"># Compute histogram</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>              <span class="c"># ... convert to probability,</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">prob</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>     <span class="c"># ... and plot probability.</span>
<span class="n">lbda</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">ISIsLow</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                 <span class="c"># Compute best guess for lambda,</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lbda</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lbda</span> <span class="o">*</span> <span class="n">bins</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c"># ... build the model,</span>
<span class="n">plot</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>                    <span class="c"># ... and plot it</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">])</span>                       <span class="c"># ... xlim from 0 to 150 ms,</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'ISI [s]'</span><span class="p">)</span>                         <span class="c"># ... label the x-axis,</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Probability'</span><span class="p">)</span>                     <span class="c"># ... and the y-axis</span>
<span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_141_0.png" alt="png" /></p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Compare the model fit to the empirical ISI distribution for the low-light condition. Does the model fit the data?</p>

<p><strong>A.</strong> No, the model does not provide a very good fit to the data. Since Poisson processes have spikes that are independent of past activity, they do not capture either the refractoriness (i.e., the few spikes observed at short times) or the bursting (i.e., the increased spiking at times 5–20 ms) that we observe in the data.</p>

<p>&lt;/div&gt;</p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Repeat the analysis and compare the empirical ISI histogram to the best-fit model in the high-light condition. Does the model fit the data?</p>

<p>&lt;/div&gt;</p>

<p>To go beyond visual inspection of the model fits and quantify the goodness of fit, we compare the cumulative distributions computed from the data and model. The <em>cumulative distribution function</em> (CDF), $F(x)$, is the probability that a random variable will take on a value less than or equal to $x$. For the exponential ISI model with rate parameter $\lambda$, the model CDF is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
F_{mod}(x) &=& \Pr(\mbox{ISI} \leq x) \\
&=& \int_0^x \lambda e^{-\lambda t} dt \\
&=& 1 - e^{-\lambda x}.
\end{eqnarray} %]]></script>

<p>We compare this to the empirical CDF of the data, $F_{emp}(x)$, which is defined as the proportion of observations less than or equal to $x$. The code to compute and plot these CDFs for the low light-condition is as follows:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>     <span class="c"># Define 1 ms bins for histogram</span>
<span class="n">lbda</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">ISIsLow</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>           <span class="c"># Compute best guess for lbda,</span>
<span class="n">FmodLow</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lbda</span> <span class="o">*</span> <span class="n">bins</span><span class="p">)</span>  <span class="c"># ... and define model CDF.</span>
<span class="n">FempLow</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>           <span class="c"># Define empirical CDF</span>
<span class="n">plot</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">FmodLow</span><span class="p">)</span>                 <span class="c"># Plot the model CDF,</span>
<span class="n">plot</span><span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">FempLow</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>       <span class="c"># ... and the empirical CDF,</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>                  <span class="c"># ... with specified x-limits</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Time [s]'</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'CDF'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_146_0.png" alt="png" /></p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Have you used the function <code class="highlighter-rouge">cumsum()</code> before? If not, look it up using <code class="highlighter-rouge">np.cumsum?</code></p>

<p>&lt;/div&gt;</p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Compare the model and empirical CDFs in the plot above. What do you think?</p>

<p><strong>A.</strong> If the model were a perfect fit, the red and blue curves would align. However, that’s not what we find here. We conclude that the model may not provide a good fit to the data.</p>

<p>&lt;/div&gt;</p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Compare the model and empirical CDF for the data in the high-light condition. What do you find? Is the model a good fit to the data?</p>

<p>&lt;/div&gt;</p>

<p>Another common way to visualize the difference between the model and empirical distributions is a <em>Kolmogorov-Smirnov</em> (KS) plot. This is just a plot of the empirical CDF against the model CDF directly.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">plot</span><span class="p">(</span><span class="n">FmodLow</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">FempLow</span><span class="p">)</span>    <span class="c"># Plot the model vs empirical CDFs.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>         <span class="c"># Set the axes ranges.</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Model CDF'</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Empirical CDF'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_151_0.png" alt="png" /></p>

<p>Since the KS plot compares CDFs, both the $x$-axis and $y$-axis range from 0 to 1. A perfect fit between the model and empirical CDFs would look like a straight, 45-degree line between the points (0,0) and (1,1). Any deviation from this line represents deviation between the observed and model distributions. One nice result for comparing CDFs is that with enough data, the maximum difference between the model and empirical CDFs has a known asymptotic distribution, which can be used to put confidence bounds about the KS plot [<a href="https://www.springer.com/us/book/9781461496014">Kass, Eden &amp; Brown, 2014</a>]. For 95% confidence bounds, a well-fit model should stay within ±1.36/$\sqrt N$ of the 45-degree line, where $N$ is the number of ISIs observed. Let’s place these confidence bounds on the KS plot:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Nlow</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>  <span class="c"># Length of the low-light condition</span>
<span class="c"># Plot the confidence bounds</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span> <span class="o">+</span> <span class="mf">1.36</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Nlow</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="s">'k:'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span> <span class="o">-</span> <span class="mf">1.36</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Nlow</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="s">'k:'</span><span class="p">)</span>
<span class="n">fig</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_153_0.png" alt="png" /></p>

<p>A well-fit model should stay entirely within these bounds. In this case, the KS plot for the low-light condition extends well outside these bounds. The exponential ISI model—or equivalently, the Poisson process model—does not fit the data in the low-light condition well. This suggests that we need a better model if we want to make meaningful comparisons about differences in the structure of the data between the two conditions.</p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Compute the KS plto with 95% significance bounds or the high-light condition. Does the exponential ISI model fit the data well?</p>

<p>&lt;/div&gt;</p>

<h4 id="a-more-advanced-statistical-model">A More Advanced Statistical Model.</h4>
<p>We’ve now investigated one class of models, the exponential distribution, to fit the observed ISI distributions. However, through analysis, we’ve found that this statistical model is not sufficient to mimic the observed data. There are many other choices for statistical models; let’s try one other class of models. The inverse Gaussian probability model has already been used successfully to describe ISI structure in this system (missing reference). The mathematical expression for the inverse Gaussian probability density is</p>

<script type="math/tex; mode=display">f(x) = \sqrt{\frac{\lambda}{2 \pi x^3}}\exp\left(\frac{-\lambda(x - \mu)^2}{2 x \mu^2}\right)
\tag{10}</script>

<p>The inverse Gaussian distribution has two parameters that determine its shape: $\mu$, which determines the mean of the distribution, and $\lambda$, which is called the shape parameter. At $x$ = 0, the inverse Gaussian has a probability density equal to zero, which suggests it could capture some of the refractoriness seen in the data.</p>

<p>If we again assume that the ISIs are independent of each other, then the likelihood of observing the sequence of ISIs, $x_1 , x_2 , . . . , x_n$, is the product of the probabilities of each ISI,</p>

<script type="math/tex; mode=display">L(\mu, \lambda) = f(x_1, x_2, ..., x_n) = \prod_{i=1}^N\sqrt{\frac{\lambda}{2\pi x_i^3}}\exp\left(\frac{-\lambda(x_i - \mu)^2}{2 x_i \mu^2}\right)
\tag{11}</script>

<p>The log likelihood is then</p>

<script type="math/tex; mode=display">\log\big(L(\mu, \lambda)\big) = \frac{N}{2}\log{\lambda}{2\pi} - \frac{3}{2}\sum_{i=1}^N \log{x_i} - \sum_{i=1}^N\frac{\lambda(x_i - \mu)^2}{2x_i \mu^2}.
\tag{12}</script>

<p>Since this distribution has two parameters, the maximum likelihood solution for this model is the pair of parameter estimates $\hat\mu$, $\hat\lambda$ that maximizes the likelihood of the data. We can solve for the maximum likelihood estimate analytically by taking the derivative with respect to both parameters, setting these equal to zero, and solving the resulting set of equations. In this case, the maximum likelihood estimators are</p>

<script type="math/tex; mode=display">\hat\mu = \frac{1}{N}\sum_{i=1}^N x_i
\tag{13}</script>

<p>and</p>

<script type="math/tex; mode=display">\hat\lambda = \left( \frac{1}{N}\sum_{i=1}^N\left(\frac{1}{x_i} - \frac{1}{\bar \mu}\right)\right)^{-1}.
\tag{14}</script>

<p>Using this expression, we can fit an inverse Gaussian model to the data in each condition and evaluate the goodness-of-fit of the model. Let’s do so now for the low-light condition.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>    <span class="c"># Define 1 ms bins.</span>
<span class="n">Nlow</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>               <span class="c"># Length of low-light condition.</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">ISIsLow</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>               <span class="c"># Mean of inverse Gaussian</span>
<span class="n">lbda</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">ISIsLow</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">mu</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>    <span class="c"># ... and shape parameter</span>
<span class="n">model</span> <span class="o">=</span> <span class="p">(</span>                                   <span class="c"># ... to create the model.</span>
    <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">lbda</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">bins</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> 
    <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lbda</span> <span class="o">*</span> <span class="p">(</span><span class="n">bins</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span>       
           <span class="mi">2</span> <span class="o">/</span> <span class="n">mu</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">bins</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>                      <span class="c"># Numerator to 0 faster than denominator.</span>
<span class="k">print</span><span class="p">(</span><span class="s">'mu = '</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>                <span class="c"># Display the MLEs</span>
<span class="k">print</span><span class="p">(</span><span class="s">'lambda = '</span><span class="p">,</span> <span class="n">lbda</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mu =  0.039988397284383186
lambda =  0.04931816769253932

</code></pre></div></div>

<p>&lt;div class=python-note&gt;</p>

<p>Note that the first element of <code class="highlighter-rouge">bins</code> is 0, so dividing by <code class="highlighter-rouge">bins</code> causes a divide by zero warning.</p>

<p>&lt;/div&gt;</p>

<p>From the computations, we find maximum likelihood estimates $\mu$ = 40.0 ms and $\lambda$ = 49.3 ms in the low-light condition. Next, we plot the data and the model.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plot the data and the model,</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">counts</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>  <span class="c"># Compute histogram,</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ISIsLow</span><span class="p">)</span>             <span class="c"># ... convert to probability,</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">prob</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>     <span class="c"># ... and plot probability</span>
<span class="n">plot</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>                   <span class="c"># Plot the model</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>                       <span class="c"># xlim from 0 to 200 ms.</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'ISI [s]'</span><span class="p">)</span>                        <span class="c"># Label the axes</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Probability'</span><span class="p">)</span>

<span class="c"># Plot the KS plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">FmodLow</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">model</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>          <span class="c"># Define the model CDF,</span>
<span class="n">FempLow</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>                <span class="c"># ... and define empirical CDF,</span>
<span class="n">plot</span><span class="p">(</span><span class="n">FmodLow</span><span class="p">,</span> <span class="n">FempLow</span><span class="p">)</span>                   <span class="c"># ... plot model vs empirical CDF,</span>
<span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.36</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Nlow</span><span class="p">))</span>  <span class="c"># ... upper confidence bound,</span>
<span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.36</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Nlow</span><span class="p">))</span>  <span class="c"># ... lower confidence bound,</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>                   <span class="c"># ... set the axes ranges,</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">'Model CDF'</span><span class="p">)</span>                      <span class="c"># ... and label the axes.</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">'Empirical CDF'</span><span class="p">)</span>
<span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/08/basic-visualizations-and-descriptive-statistics-of-spike-train-data_160_0.png" alt="png" /></p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Consider the fit of the inverse Gaussian model to the data in the low-light condition. Does the inverse Gaussian model provide a good fit to the ISIs?</p>

<p><strong>A.</strong> This model provides a much better fit to the data; the KS plot is contained within the 95% confidence bounds.</p>

<p>&lt;/div&gt;</p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Consider the fit of the inverse Gaussian model to the data in the high-light con- dition. Does the inverse Gaussian model provide a good fit to these ISIs?</p>

<p>&lt;/div&gt;</p>

<p>&lt;div class=question&gt;</p>

<p><strong>Q.</strong> Compare the estimates of the two parameters $\mu$ and $\lambda$ of the inverse Gaussian model in the two conditions. What do these reveal about the differences between the low- and high-light conditions?</p>

<p>&lt;/div&gt;</p>

<p><a href="#top">Back to top</a>
&lt;a id=summary&gt;&lt;/a&gt;</p>
<h2 id="summary">Summary</h2>
<p>In this chapter, we considered the spiking activity recorded in two conditions. We began with visualizations of the spiking data, and construction and visualization of the increment process (i.e., binned spike counts). We then assessed the variability in the increments through computation of the Fano factor, and showed that the low- and high-light conditions had less and more variability, respectively, than expected for a Poisson process. We also assessed the autocorrelation of the increment processes and observed the impact of refractoriness and bursting activity. In addition, we created and visualized the ISIs for each condition. Inspection of the ISI histograms suggested bursting activity in both conditions, and more small ISIs in the high-light condition. Analysis of the ISI autocorrelations revealed no compelling evidence for dependence and supported the hypothesis of a renewal process. Finally, we built two statistical models of the observed ISIs. We discussed how to fit the model parameters by computing the maximum likelihood estimate, and how to evaluate the model goodness-of-fit to the data using the KS plot. We showed that the first model—the Poisson process as a model of spiking with a corresponding exponential distribution of ISIs—did not fit the observed ISI data. A second model—the inverse Gaussian probability model—provided a much more accurate fit to the observed ISIs. The modeling suggests that at least two features of the spiking activity have changed from the low-light to the high-light condition. First, the mean ISI is smaller, and hence the average firing rate is larger, in the high-light condition. Second, the shape of the firing distribution has changed so that the cell is more likely to fire in bursts with short ISIs in the high-light condition.</p>

<p><a href="#top">Back to Top</a>
&lt;a id=appendix&gt;&lt;/a&gt;</p>
<h2 id="appendix-spike-count-mean-and-variance-for-a-poisson-process">Appendix: Spike Count Mean and Variance for a Poisson Process</h2>

<p>In this appendix, we compute the theoretical mean $\mu$ and the theoretical variance of the spike count $\sigma^2$ for a Poisson process. Let’s compute $\mu$ using a general formula that makes use of the probability $P(k)$ of observing $k$ spikes,
&lt;a id=eq:15&gt;&lt;/a&gt;</p>

<script type="math/tex; mode=display">\mu = \sum_{k=1}^\infty k P(k).</script>

<p>Replacing $P(k)$ with the expression for a Poisson distribution 
(<a href="#eq:6" class="thumb">eq.<span><img src="imgs/eq6.png" /></span></a>)
, we find</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\mu &=& \sum_{k=0}^\infty k\left(\frac{\lambda^k e^{-k}}{k!}\right) \\
&=& e^{-\lambda}\sum_{k=0}^\infty k\frac{\lambda^k}{k!}.
\end{eqnarray} %]]></script>

<p>To make progress, let’s write out the terms in the summation,
&lt;a id=eq:16&gt;&lt;/a&gt;</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\mu &=& e^{-\lambda}
\left( 
0 + 
\frac{\lambda^1}{1!} + 
2\frac{\lambda^2}{2!} + 
3\frac{\lambda^3}{3!} + 
4\frac{\lambda^4}{4!} + 
\cdots 
\right) \\
&=& e^{-\lambda}\lambda
\left(
1 + 
\frac{\lambda^2}{2!} + 
\frac{\lambda^3}{3!} + 
\cdots 
\right) \\
&=& e^{-\lambda}\lambda(e^\lambda) \\
&=& \lambda,
\end{eqnarray} %]]></script>

<p>where we have used the fact that $e^x = 1 + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \cdots$. Notice that the mean spike count equals the rate parameter of the Poisson process.</p>

<p>To find the spike count variance for a Poisson process, we follow a similar procedure. In general, we compute the variance $\sigma^2$ of the spike count $k$ with probability distribution $P(k)$ as 
&lt;a id=eq:17&gt;&lt;/a&gt;</p>

<script type="math/tex; mode=display">\sigma^2 = \sum_{k=0}^\infty k^2 P(k) - \left(\sum_{k=0}^\infty k P(k)\right) ^2.</script>

<p>As in our computation of the mean spike count, we replace $P(k)$ with (&lt;a href=#eq:6 class=thumb&gt;eq.<span>&lt;img src=imgs/eq6.png&gt;</span>&lt;/a&gt;)
, the expression for a Poisson process. Notice that the second term is the square of the expression (&lt;a href=#eq:15 class=thumb&gt;eq.<span>&lt;img src=imgs/eq15.png&gt;</span>&lt;/a&gt;)
, and for a Poisson process we found $\mu = \lambda$. Therefore, let’s replace the second term in (&lt;a href=#eq:17 class=thumb&gt;eq.<span>&lt;img src=imgs/eq17.png&gt;</span>&lt;/a&gt;)
 with $\sigma^2$ and substitute for $P(k)$ in the first term of (&lt;a href=#eq:17 class=thumb&gt;eq.<span>&lt;img src=imgs/eq17.png&gt;</span>&lt;/a&gt;) to find</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\sigma^2 &=&
\sum_{k=0}^\infty
k^2
\left(
\frac{\lambda^k e^{-\lambda}}{k!}
\right)
- \lambda^2
\\
&=&
e^{-\lambda}
\sum_{k=0}^\infty
k^2
\frac{\lambda^k}{k!} 
- \lambda^2.
\end{eqnarray} %]]></script>

<p>To make progress, we follow the same strategy and write out the terms in the summation,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\sigma^2
&=&
e^{-\lambda}
\left(
0 + 
\lambda +
2^2\frac{\lambda^2}{2!} + 
3^2\frac{\lambda^3}{3!} + 
4^2\frac{\lambda^4}{4!} + 
\cdots
\right) - 
\lambda^2
\\
&=&
\lambda e ^{-\lambda}
\left(
1 + 
2\lambda + 
\frac{3}{2}\lambda ^2 + 
\frac{4}{6}\lambda ^2 + 
\cdots
\right) - 
\lambda^2.
\end{eqnarray} %]]></script>

<p>Now, we divide this sum of terms into two pieces, a “nice term” (in the first brackets) and “leftovers” (in the second brackets):</p>

<script type="math/tex; mode=display">\sigma^2 = \lambda e^{-\lambda}
\left(
\left[
1 + 
\lambda + 
\frac{\lambda^2}{2!} + 
\frac{\lambda^3}{3!} + 
\cdots
\right] + 
\left[
\lambda + 
\frac{2\lambda^2}{2!} + 
\frac{3\lambda^3}{3!} + 
\cdots
\right] 
\right) - 
\lambda^2.</script>

<p>We can simplify by recognizing that 
$\left[
1 + 
\lambda + 
\frac{\lambda^2}{2!} + 
\frac{\lambda^3}{3!} + 
\cdots
\right]
=
e^\lambda$.
Then
&lt;a id=eq:18&gt;&lt;/a&gt;</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\sigma^2
&=&
\lambda e^{-\lambda}
\left(
e^\lambda + 
\lambda
\left[
1 + 
\lambda + 
\frac{\lambda^1}{1!} + 
\frac{\lambda^2}{2!} + 
\cdots
\right] 
\right)-
\lambda^2 \\
&=&
\lambda e^{-\lambda}
\left(
e^\lambda + \lambda e^\lambda
\right)
-\lambda^2 \\
&=&
\lambda + \lambda^2 - \lambda^2 \\
&=&
\lambda,
\end{eqnarray} %]]></script>

<p>where again we’ve used the definition of $e^\lambda$. We conclude that the spike count variance for a Poisson precess equals the firing rate $\lambda$.</p>

<p>Combining these results for the mean spike count $\mu$ 
(&lt;a href=#eq:16 class=thumb&gt;eq.<span>&lt;img src=imgs/eq16.png&gt;</span>&lt;/a&gt;)
and the spike count variance
(&lt;a href=#eq:18 class=thumb&gt;eq.<span>&lt;img src=imgs/eq18.png&gt;</span>&lt;/a&gt;),
we conclude that for a Poisson process,</p>

<script type="math/tex; mode=display">\mu = \sigma^2 = \lambda,</script>

<p>and therefore for a Poisson process, the Fano factor $\sigma^2/\mu=1$.</p>

<p><a href="#top">Back to top</a></p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s">'../style.css'</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_html">
<style>
.math-note {
    color: #3c763d;
    background-color: #dff0d8;
    border-color: #d6e9c6;
    padding: 15px;
    margin-bottom: initial;
    border: 1px solid transparent;
    border-radius: 2px;
}
.python-note {
    margin-bottom: initial;
    color: #8a6d3b;
    background-color: #fcf8e3;
    border-color: #faebcc;
    padding: 15px;
    border: 1px solid transparent;
    border-radius: 2px;
}
.question {
    color: #31708f;
    background-color: #d9edf7;
    border-color: #bce8f1;
    padding: 15px;
    margin-bottom: initial;
    border: 1px solid transparent;
    border-radius: 2px;
}
.output_area img {
    display: block;
    margin-left: auto;
    margin-right: auto;
}
.inner_cell img {
	width:100%;
	max-width:500px;
}
.thumb img { 
	border:1px solid #000;
	margin:0px;
	float:center;
    background:#fff;
}
.thumb span { 
	visibility: hidden;
    width: 300px;
    background-color: black;
    color: #fff;
    text-align: center;
    border-radius: 6px;
    padding: 5px 5px;
    position: fixed;
    z-index: 1;
    bottom: 50%;
    left: 50%;
    margin-left: -150px;
    transition: 5ms visibility;
}
.thumb:hover, .thumb:hover span { 
	visibility:visible;
    transition-delay: 500ms;
		
}    
.fig img { 
	border:1px solid #000;
	margin:0px;
	float:center;
    background:#fff;
}
.fig span { 
	visibility: hidden;
    width: 500px;
    background-color: black;
    color: #fff;
    text-align: center;
    border-radius: 6px;
    padding: 5px 5px;
    position: fixed;
    z-index: 1;
    bottom: 40%;
    left: 50%;
    margin-left: -250px;
    transition: 5ms visibility;
}
.fig:hover, .fig:hover span { 
	visibility:visible;
    transition-delay: 500ms;
}
</style>


</div>

<ol class="bibliography"><li><span id="holdgraf_evidence_2014">Holdgraf, C. R., de Heer, W., Pasley, B. N., &amp; Knight, R. T. (2014). Evidence for Predictive Coding in Human Auditory Cortex. In <i>International Conference on Cognitive Neuroscience</i>. Brisbane, Australia, Australia: Frontiers in Neuroscience.</span></li>
<li><span id="holdgraf_rapid_2016">Holdgraf, C. R., de Heer, W., Pasley, B. N., Rieger, J. W., Crone, N., Lin, J. J., … Theunissen, F. E. (2016). Rapid tuning shifts in human auditory cortex enhance speech intelligibility. <i>Nature Communications</i>, <i>7</i>(May), 13654. https://doi.org/10.1038/ncomms13654</span></li>
<li><span id="holdgraf_portable_2017">Holdgraf, C. R., Culich, A., Rokem, A., Deniz, F., Alegro, M., &amp; Ushizima, D. (2017). Portable learning environments for hands-on computational instruction using container-and cloud-based technology to teach data science. In <i>ACM International Conference Proceeding Series</i> (Vol. Part F1287). https://doi.org/10.1145/3093338.3093370</span></li>
<li><span id="holdgraf_encoding_2017">Holdgraf, C. R., Rieger, J. W., Micheli, C., Martin, S., Knight, R. T., &amp; Theunissen, F. E. (2017). Encoding and decoding models in cognitive electrophysiology. <i>Frontiers in Systems Neuroscience</i>, <i>11</i>. https://doi.org/10.3389/fnsys.2017.00061</span></li>
<li><span id="ruby">Flanagan, D., &amp; Matsumoto, Y. (2008). <i>The Ruby Programming Language</i>. O’Reilly Media.</span></li></ol>

              <nav class="c-page__nav">
  
    
    <a id="js-page__nav__prev" class="c-page__nav__prev" href="/Case-Studies-Python/06/the-power-spectrum-part-2">
      〈 <span class="u-margin-right-tiny"></span> The Power Spectrum (Part 2)
    </a>
  

  
    
    <a id="js-page__nav__next" class="c-page__nav__next" href="">
       <span class="u-margin-right-tiny"></span> 〉
    </a>
  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
